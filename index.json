[{"categories":[""],"content":"最近使用PyYAML处理YAML文件，做个记录。 ","date":"2020-08-30","objectID":"/posts/2020/08/30/pyyaml/:0:0","tags":["YAML","PyYAML","Python"],"title":"PyYAML笔记","uri":"/posts/2020/08/30/pyyaml/"},{"categories":[""],"content":"关于Node 在PyYAML中，Node作为YAML信息模型的实体，有三种类型： 标量scalar ScalarNode(tag, value, style, start_mark, end_mark) 序列sequence SequenceNode(tag, value, flow_style, start_mark, end_mark) 映射mapping MappingNode(tag, value, flow_style, start_mark, end_mark) node由Composer产生，通过Serializer可将其序列化为YAML流。 示例代码： content = \"\"\" title: 这是一个标题 categories: - 分类1 tags: - 标签1 - 标签2 \"\"\" node = yaml.compose(content) print(node) print('') stream = yaml.serialize(node) print(stream) 输出： MappingNode(tag='tag:yaml.org,2002:map', value=[(ScalarNode(tag='tag:yaml.org,2002:str', value='title'), ScalarNode(tag='tag:yaml.org,2002:str', value='这是一个标题')), (ScalarNode(tag='tag:yaml.org,2002:str', value='categories'), SequenceNode(tag='tag:yaml.org,2002:seq', value=[ScalarNode(tag='tag:yaml.org,2002:str', value='分类1')])), (ScalarNode(tag='tag:yaml.org,2002:str', value='tags'), SequenceNode(tag='tag:yaml.org,2002:seq', value=[ScalarNode(tag='tag:yaml.org,2002:str', value='标签1'), ScalarNode(tag='tag:yaml.org,2002:str', value='标签2')]))]) title: \"\\u8FD9\\u662F\\u4E00\\u4E2A\\u6807\\u9898\" categories: - \"\\u5206\\u7C7B1\" tags: - \"\\u6807\\u7B7E1\" - \"\\u6807\\u7B7E2\" ","date":"2020-08-30","objectID":"/posts/2020/08/30/pyyaml/:1:0","tags":["YAML","PyYAML","Python"],"title":"PyYAML笔记","uri":"/posts/2020/08/30/pyyaml/"},{"categories":[""],"content":"关于constructor和representer constructor之于Loading，representer之于Dumping，二者功能相反。 ","date":"2020-08-30","objectID":"/posts/2020/08/30/pyyaml/:2:0","tags":["YAML","PyYAML","Python"],"title":"PyYAML笔记","uri":"/posts/2020/08/30/pyyaml/"},{"categories":[""],"content":"constructor 用于将一个YAML节点转换为Python对象。 yaml.add_constructor(tag, constructor, Loader=Loader)为目标tag指定constructor；constructor的参数包括一个Loader实例和一个YAML节点，返回值是一个Python对象。 Loader提供了获取YAML节点值的方法（该值用于构造Python对象）： Loader.construct_scalar(node) 验证目标YAML节点是标量节点scalar node并返回其值 Loader.construct_sequence(node) 验证目标YAML节点是序列节点sequence node并返回其值 Loader.construct_mapping(node) 验证目标YAML节点是映射节点mapping node并返回其值 示例代码： class Post(object): def __init__(self, title, categories, tags): self.title = title self.categories = categories self.tags = tags def post_constructor(loader, node): mapping_value = loader.construct_mapping(node) return Post(mapping_value['title'], mapping_value['categories'], mapping_value['tags']) yaml.add_constructor(u'!Post', post_constructor) res = yaml.load(\"\"\" !Post title: 这是一个标题 categories: - 分类1 tags: - 标签1 - 标签2 \"\"\") print(type(res)) print(res.title) print(res.categories) print(res.tags) 输出： \u003cclass '__main__.Post'\u003e 这是一个标题 ['分类1'] ['标签1', '标签2'] ","date":"2020-08-30","objectID":"/posts/2020/08/30/pyyaml/:2:1","tags":["YAML","PyYAML","Python"],"title":"PyYAML笔记","uri":"/posts/2020/08/30/pyyaml/"},{"categories":[""],"content":"representer 用于将一个Python对象转换为YAML节点。 yaml.add_representer(data_type, representer, Dumper=Dumper)为指定类型的对象指定representer；representer的参数包含一个Dumper实例和一个Python对象，返回值是一个YAML节点。 Dumper提供了构造YAML节点的方法： Dumper.represent_scalar(tag, value, style=None) 返回具有指定tag和value的标量节点scalar node Dumper.represent_sequence(tag, sequence, flow_style=None) 返回具有指定tag和value的序列节点sequence node Dumper.represent_mapping(tag, mapping, flow_style=None) 返回具有指定tag和value的映射节点mapping node 示例代码： class Post(object): def __init__(self, title, categories, tags): self.title = title self.categories = categories self.tags = tags def post_representer(dumper, post): attrs = [] attrs.append(('title', post.title)) attrs.append(('categories', post.categories)) attrs.append(('tags', post.tags)) return dumper.represent_mapping(u'!Post', attrs) post = Post('这是一个标题', ['分类1'], ['标签1', '标签2']) yaml.add_representer(Post, post_representer) res = yaml.dump(post, allow_unicode=True) print(res) 输出： !Post title: 这是一个标题 categories: - 分类1 tags: - 标签1 - 标签2 可以看到输出内容带有类型tag，如果希望不附带tag，需使用YAML标准类型，在本例中dumper.represent_mapping(u'tag:yaml.org,2002:map', attrs)。 YAML标准类型tag可以参考 https://yaml.org/type/index.html 。 ","date":"2020-08-30","objectID":"/posts/2020/08/30/pyyaml/:2:2","tags":["YAML","PyYAML","Python"],"title":"PyYAML笔记","uri":"/posts/2020/08/30/pyyaml/"},{"categories":[""],"content":"关于yaml.dump()参数 dump(data, stream=None, Dumper=Dumper, default_style=None, default_flow_style=None, encoding='utf-8', # encoding=None (Python 3) explicit_start=None, explicit_end=None, version=None, tags=None, canonical=None, indent=None, width=None, allow_unicode=None, line_break=None) 已知参数用法： allow_unicode=True 正常显示Unicode字符，比如中文 default_flow_style=True 紧凑输出为一行 canonical=True 以Canonical Form格式输出 ","date":"2020-08-30","objectID":"/posts/2020/08/30/pyyaml/:3:0","tags":["YAML","PyYAML","Python"],"title":"PyYAML笔记","uri":"/posts/2020/08/30/pyyaml/"},{"categories":[""],"content":"关于Tag YAML使用tag标识数据类型，全局tag采用tag:起始的URL格式，本地tag采用!起始的格式，格式说明参考下面官方文档： Tag property: # Usually unspecified. none : Unspecified tag (automatically resolved by application). '!' : Non-specific tag (by default, \"!!map\"/\"!!seq\"/\"!!str\"). '!foo' : Primary (by convention, means a local \"!foo\" tag). '!!foo' : Secondary (by convention, means \"tag:yaml.org,2002:foo\"). '!h!foo': Requires \"%TAG !h! \u003cprefix\u003e\" (and then means \"\u003cprefix\u003efoo\"). '!\u003cfoo\u003e': Verbatim tag (always means \"foo\") 其中，!!是tag:yaml.org,2002:的缩写，dump内容所包含的tag采用缩写格式。 参考文档 https://pyyaml.org/wiki/PyYAMLDocumentation https://yaml.org/type/index.html https://yaml.org/refcard.html ","date":"2020-08-30","objectID":"/posts/2020/08/30/pyyaml/:4:0","tags":["YAML","PyYAML","Python"],"title":"PyYAML笔记","uri":"/posts/2020/08/30/pyyaml/"},{"categories":["系统工具"],"content":"最终还是投入Hugo的怀抱，原因是： 熟悉和欣赏Go 发现一个超级喜欢的Hugo主题 - LoveIt 本文记录下博客从Jekyll迁移到Hugo的过程。 ","date":"2020-08-29","objectID":"/posts/2020/08/29/migrating-from-jekyll-to-hugo/:0:0","tags":["Hugo","Jekyll","静态博客"],"title":"从Jekyll迁移到Hugo","uri":"/posts/2020/08/29/migrating-from-jekyll-to-hugo/"},{"categories":["系统工具"],"content":"创建项目 安装Hugo sudo pacman -S hugo 创建名称为Demo的新项目 hugo new site Demo 添加主题（此为必需步骤，可以选择其他主题） cd Demo \u0026\u0026 git init \u0026\u0026 git submodule add https://github.com/dillonzq/LoveIt.git themes/LoveIt 也可以自行下载主题，放置于 Demo/themes 目录下 但作为submodule添加进Git项目是一个更好的方式 创建示例文章 hugo new posts/first.md 产生的文件位于是 Demo/content/posts/first.md 启动Hugo服务 hugo server 本地预览访问 http://localhost:1313/ ","date":"2020-08-29","objectID":"/posts/2020/08/29/migrating-from-jekyll-to-hugo/:1:0","tags":["Hugo","Jekyll","静态博客"],"title":"从Jekyll迁移到Hugo","uri":"/posts/2020/08/29/migrating-from-jekyll-to-hugo/"},{"categories":["系统工具"],"content":"文章迁移 Hugo提供了迁移命令import，可以方便地将Jekyll项目转换为Hugo项目，包括文章和其他静态资源文件。 命令格式是hugo import jekyll JEKYLL_ROOT_PATH TARGET_PATH，其中 JEKYLL_ROOT_PATH 是Jekyll项目主目录，TARGET_PATH 是新生成的Hugo项目目录。 以我的一篇文章为例，原Jekyll项目中的位置是 _post/2020-07-29-cpp-template-notes.md，Hugo项目中的位置是 content/post/2020-07-29-cpp-template-notes.md，需要注意的是，迁移前后文件名保持一致，划重点后面用到。 Jekyll Front Matter ---layout:posttitle:C++类与模板categories:[编程语言]tags:[C++,Template]--- Hugo Front Matter ---categories:- 编程语言date:\"2020-07-29T00:00:00Z\"tags:- C++- Templatetitle:C++类与模板--- 同时需要注意，Jekyll和Hugo在FrontMatter变量的定义有区别，划重点。 Jekyll Hugo date post文件名或FrontMatter指定，FrontMatter重写前者 FrontMatter指定 title post文件名指定 FrontMatter指定，即文章标题 filename N/A 即文件名（不包含扩展名） section N/A content下的目录名 以 2020-07-29-cpp-template-notes.md 为例， 对于Jekyll，date是 2020-07-29，title是 cpp-template-notes； 对于Hugo，date是 2020-07-29，title是 C++类与模板，filename是 2020-07-29-cpp-template-notes。 ","date":"2020-08-29","objectID":"/posts/2020/08/29/migrating-from-jekyll-to-hugo/:2:0","tags":["Hugo","Jekyll","静态博客"],"title":"从Jekyll迁移到Hugo","uri":"/posts/2020/08/29/migrating-from-jekyll-to-hugo/"},{"categories":["系统工具"],"content":"URL规则 URL规则在全局配置文件（config.toml）的 permalinks 进行配置，也可以使用文章HugoFrontMatter的 url 变量指定（优先级高于前者）。 需要保证，无论URL规则是否变化，都能通过原始URL访问到文章。 对于迁移后的三种情形： 保持原有URL规则 如果原Jekyll的permalink配置使用 title 变量，相应地在Hugo下使用 filename 变量，由于迁移前后文件名保持一致，这将破坏原有的URL规则，可以采取两个方案： 使用HugoFrontMatter的 url 变量指定原始URL 去除迁移后文件名的date部分 个人采用第一种，因为通过文件名可以区分哪些是迁移前的文章。 老文章保持原有URL规则，新文章采用新的URL规则 老文章使用HugoFrontMatter的 url 变量指定原始URL，新文章采用全局 permalinks 配置，这种方式实现起来最简单，但URL规则不统一，看着很不规范，不推荐！ 采用新的URL规则 如果原Jekyll的permalink配置使用 title 变量，使用HugoFrontMatter的 url 变量指定新URL； 使用HugoFrontMatter的 aliases 变量指定原始URL，实现页面重定向。 对于我的博客，原Jekyll的URL规则是 permalink:/:year/:month/:day/:title 在Hugo下使用了新的URL规则 [permalinks] posts = \"/:section/:year/:month/:day/:filename/\" JekyllFrontMatter ---layout:posttitle:C++类与模板categories:[编程语言]tags:[C++,Template]--- HugoFrontMatter ---title:C++类与模板categories:- 编程语言tags:- C++- Templatedate:2020-07-29 00:00:00+08:00url:/posts/2020/07/29/cpp-template-notes/aliases:- /2020/07/29/cpp-template-notes/--- Hugo import命令无法满足我的场景，而且HugoFrontMatter变量按照首字符顺序排序，看着实在难受，本着轮子能造就造的原则，写了一个转换工具，仅对_posts下文章进行转换，有需要可以尝试下，项目地址是 https://github.com/caosiyang/convert-jekyll-to-hugo ，嗯，这个硬广阔以：） ","date":"2020-08-29","objectID":"/posts/2020/08/29/migrating-from-jekyll-to-hugo/:3:0","tags":["Hugo","Jekyll","静态博客"],"title":"从Jekyll迁移到Hugo","uri":"/posts/2020/08/29/migrating-from-jekyll-to-hugo/"},{"categories":["系统工具"],"content":"GitHub Pages发布 执行hugo命令，在 public 目录下生成静态网站，push里面的内容到github就可以了。 Hugo官方提供的方案是submodule方式，如下： $ rm -rf public $ git submodule add -b master https://github.com/\u003cUSERNAME\u003e/\u003cUSERNAME\u003e.github.io.git public $ hugo 发布新的Hugo页面时遇到了问题，顺便说一下吧。 之前我的博客项目已经托管在GitHub，我把原repo重命名了（暂且称“A”），新建一个repo（暂且称为“B”），名字是.github.io，然后push页面，之后浏览，“首页/分类/标签/关于”页面都是正常的，而“文章”页面还是原始的Jekyll生成的页面，初以为是缓存问题，过了一晚还这样，怀疑跟A有管，把A删除，问题仍然存在，之后又把B删除，重新建repo和push，恢复正常。 ","date":"2020-08-29","objectID":"/posts/2020/08/29/migrating-from-jekyll-to-hugo/:4:0","tags":["Hugo","Jekyll","静态博客"],"title":"从Jekyll迁移到Hugo","uri":"/posts/2020/08/29/migrating-from-jekyll-to-hugo/"},{"categories":["系统工具"],"content":"博客历程回顾 最初产生写博客的想法是2011年，试用了当时几个流行的技术博客平台（csdn/cnblogs/cppblog），最终选择了cnblogs。 2015年了解到GitHub Pages可以搭建博客，当时已经在用GitHub，所以不假思索地转过去，使用Jekyll搭建了新博客，看了很多主题，选了一个并进行小改，但并不满意，最后抱着学习前端的想法，自己写了一个相当相当粗糙的主题，凑合用着了。 后来先后了解到Hexo和Hugo，同为热门的静态网站生成工具，前者node.js实现，后者Go实现，看过它们的主题，感觉都比Jekyll的好看，也很玄学，但迟迟没有折腾，现在想来，能坚持更博就不错了。 2020年是不平凡的一年，新冠疫情爆发，我的工作生活发生了很大变化，在一段闲暇的日子里，又开始琢磨博客，在看到一个相当中意的Hugo主题，坚定了我转到Hugo的决心，于是有了现在的博客。 参考文档 https://gohugo.io/tools/migrations/ https://gohugo.io/commands/hugo_import_jekyll/ https://gohugo.io/hosting-and-deployment/hosting-on-github/ ","date":"2020-08-29","objectID":"/posts/2020/08/29/migrating-from-jekyll-to-hugo/:5:0","tags":["Hugo","Jekyll","静态博客"],"title":"从Jekyll迁移到Hugo","uri":"/posts/2020/08/29/migrating-from-jekyll-to-hugo/"},{"categories":null,"content":"博主是一个热爱开源的大龄码农。 博客由Hugo生成，使用LoveIt主题，托管于GitHub Pages。 本站原创文章遵循 创作共享 署名-非商业性 4.0 许可协议 / CC BY-NC 4.0 转载内容请注明来源（所在文章链接 或 caosiyang.github.io） ","date":"2020-08-26","objectID":"/about/:0:0","tags":null,"title":"关于","uri":"/about/"},{"categories":["编程语言"],"content":"声明和实现 一般情况下，类模板的声明和实现在一个头文件内完成。 如果采用声明实现分离的方式，在.h文件中声明，在.hpp文件中实现，使用该类模板时include目标.hpp文件。 ","date":"2020-07-29","objectID":"/posts/2020/07/29/cpp-template-notes/:1:0","tags":["C++","Template"],"title":"C++类与模板","uri":"/posts/2020/07/29/cpp-template-notes/"},{"categories":["编程语言"],"content":"类模板的继承 涉及类模板的继承有多种情况，较为常见的是 父类是特化的模板类，子类是普通类 class Derived ： public Base\u003cint\u003e {} 父类是普通类，子类是模板 template \u003ctypename T\u003e class Derived ： public Base {} 父类和子类都是模板 template \u003ctypename T\u003e class Derived : public Base\u003cT\u003e {} 如果父类和子类都是模板，子类在访问父类的数据成员和函数成员时，必须使用this-\u003e或者Base\u003cT\u003e::对目标成员进行限定，原因是编译器在编译Derived时无法确定Base类型尚且未知，所以名字解析不会检查父类范围，缺少限定符访问父类成员将报错（找不到目标成员），加上限定符后父类成员名字判定为依赖名字，二阶名字查找将在类模板实例化时解析父类成员名字。 什么是二阶名字查找？ 二阶名字查找（two-phase name lookup）是模板内部名字查找的过程，之所以称之为“二阶“，是因为模板内部名字分为依赖名字和非依赖名字，这两类名字在不同时期进行解析，此规则在1993或1994年引入C++标准草案，其作用是让名字解析更加可靠，但可能与年代久远的模板代码不兼容。 什么是依赖名字和非依赖名字？ 依赖名字（dependent names）是依赖于模板参数并且在模板中未声明的名字，其仅在模板实例化时解析； 非依赖名字（non-dependent names）是不依赖于模板参数的名字，以及模板本身的名字和其内部声明的名字（成员、友元，本地变量），其在模板定义时解析。 参考 https://stackoverflow.com/questions/7076169/not-declared-in-this-scope-error-with-templates-and-inheritance https://womble.decadent.org.uk/c++/template-faq.html https://code-examples.net/en/q/2c8d ","date":"2020-07-29","objectID":"/posts/2020/07/29/cpp-template-notes/:2:0","tags":["C++","Template"],"title":"C++类与模板","uri":"/posts/2020/07/29/cpp-template-notes/"},{"categories":["系统工具"],"content":"Manjaro Xfce使用过程遇到的一些问题和解决方法。 ","date":"2020-07-27","objectID":"/posts/2020/07/27/manjaro-xfce-notes/:0:0","tags":["Linux","Manjaro","Xfce"],"title":"Manjaro Xfce使用笔记","uri":"/posts/2020/07/27/manjaro-xfce-notes/"},{"categories":["系统工具"],"content":"Python版本 系统预装和默认使用的是Python 3（默认版本通过软连接/usr/bin/python指定）。 如果改为Python 2，会导致部分应用程序无法启动，比如说LightDM GTK+ Greeter settings。 ","date":"2020-07-27","objectID":"/posts/2020/07/27/manjaro-xfce-notes/:1:0","tags":["Linux","Manjaro","Xfce"],"title":"Manjaro Xfce使用笔记","uri":"/posts/2020/07/27/manjaro-xfce-notes/"},{"categories":["系统工具"],"content":"桌面背景和登录界面背景 桌面背景图片的目录是/usr/share/backgrounds/，可以新建目录存放个人背景图片。 如果要让桌面背景同时作为登录界面背景，可以使用LightDM GTK+ Greeter settings应用程序进行配置，其对应的配置文件是/etc/lightdm/lightdm-gtk-greeter.conf。 打开应用程序，勾选Use user wallpaper if available选项。 登录界面背景跟随桌面背景，有一个从默认greeter background到桌面背景图片的过渡过程，纯黑Color还好，否则会让这个过程看起来非常别扭，所以，建议将登录背景（Background Image）设置与桌面背景一致 如果你有多个工作区，一定要在第一个工作区执行设置桌面背景图片的操作，否则登录背景不会跟随 不勾选该选项则使用固定的登录背景，不跟随桌面背景，比较省心。 ","date":"2020-07-27","objectID":"/posts/2020/07/27/manjaro-xfce-notes/:2:0","tags":["Linux","Manjaro","Xfce"],"title":"Manjaro Xfce使用笔记","uri":"/posts/2020/07/27/manjaro-xfce-notes/"},{"categories":["系统工具"],"content":"近期入手京东京造K6键盘，其实是Keychron K6，这款键盘的亮点是Windows和MacOS双系统兼容，然而我在Manjaro系统下遇到一些问题： Windows兼容模式下，多媒体控制正常（fn1+数字），F1-F12无效（fn2+数字，错误地表现为多媒体控制） MacOS兼容模式下，多媒体控制和F1-F12均正常，但苹果键盘的option和command分别对应于PC键盘的alt和super，导致二者键位相反 先后尝试了两种方案，最后选择了第二种方案。 方案一 MacOS兼容模式，系统层面对换option和command键位，这样左侧与PC键位相同，依次是ctrl super alt。 方法是新建/etc/modprobe.d/hid_apple.conf，内容如下，然后重启系统。 options hid_apple swap_opt_cmd=1 后来发现右侧command键位是alt，因为这里我需要它是ctrl，尝试添加下面配置，无法修复此问题，所以放弃这个方案。 options hid_apple rightalt_as_rightctrl=1 方案二 Window兼容模式，修改fnmode配置。 方法是/etc/modprobe.d/hid_apple.conf，内容如下，然后重启系统。 options hid_apple fnmode=2 回顾整个过程，fnmode是一个关键点，看了资料，fnmode有3个模式： 0，禁用fn键 1，启用fn键，F1-F12是特殊功能键（多媒体控制），fn+F1-F12则是F1-F12 2，启用fn键，F1-F12即自身，fn+F1-F12则是特殊功能键（多媒体控制） 之后分别测试K6键盘在Manjaro系统不同fnmode下的表现，结论如下： fnmode 0 Win模式，均正常 Mac模式，fn1/fn2+数字都是F1-F12，多媒体控制不正常 fnmode 1 Win模式，fn1/fn2+数字都是多媒体控制正常，F1-F12不正常 Mac模式，均正常 fnmode 2 Win模式，均正常 Mac模式，fn1+数字是F1-F12，fn2+数字是多媒体控制，与说明书上的功能完全相反 题外话，说一下这款键盘的使用感受。 京东京造K6蓝牙双模机械键盘，佳达隆红轴，68键，abs键帽，白色背光（个人第一把带灯键盘）。 68键布局，有独立的方向键，右侧双fn键（分别用于F区和多媒体控制），取消了右alt键，需要吐槽的是space-ctrl-fn1-fn2键位，敲ctrl键真的是太不习惯，需要慢慢适应。 键帽手感丝滑，略轻薄（还是更喜欢PBT的厚实感），退格键较松垮并有轻微钢丝声。 一直想体验下60%配列，简洁小巧，节省空间，考虑过ikbc Poker和Anne Pro，迟迟没有下手，偶然了解到Keychron，618活动价还是挺香的，得以入手，感谢京东。 参考 https://schnouki.net/post/2019/how-to-use-a-keychron-k2-usb-keyboard-on-linux/ https://wiki.archlinux.org/index.php/Apple_Keyboard https://wiki.debian.org/InstallingDebianOn/Apple/PageFragmentKeyboard ","date":"2020-07-22","objectID":"/posts/2020/07/22/using-keychron-k6-on-manjaro/:0:0","tags":["Linux","Manjaro","机械键盘"],"title":"京东京造K6键盘在Linux下的兼容问题及解决方法","uri":"/posts/2020/07/22/using-keychron-k6-on-manjaro/"},{"categories":["系统工具"],"content":"操作系统的时间（时钟）由以下4个部分决定： 时间数值 时间标准（本地时间/UTC/其他） 时区 夏令时（DST，Daylight Saving Time，部分适用，中国已经废止） 操作系统有两个时钟，分别是硬件时钟和系统时钟，大多数操作系统的时钟管理方式如下： 启动时根据硬件时钟设置系统时钟 维护准确的系统时钟 关机时根据系统时钟设置硬件时钟 ","date":"2020-06-02","objectID":"/posts/2020/06/02/linux-clock/:0:0","tags":["Linux","硬件时钟","系统时钟","时间同步","时间标准"],"title":"Linux硬件时钟与系统时钟","uri":"/posts/2020/06/02/linux-clock/"},{"categories":["系统工具"],"content":"硬件时钟 硬件时钟，又称实时时钟（Real Time Clock，RTC）或CMOS时钟，是独立的硬件设备（电池、电容元件等），保存的时间包括年、月、日、时、分、秒。2016年之后的UEFI固件还能保存时区和是否使用夏令时。 操作硬件时钟的工具是hwclock，用于查询、设置硬件时钟等。 读取硬件时钟： # hwclock --show 使用系统时钟设置硬件时钟： # hwclock --systohc 更新硬件时钟后，/etc/adjtime随之更新。 ","date":"2020-06-02","objectID":"/posts/2020/06/02/linux-clock/:1:0","tags":["Linux","硬件时钟","系统时钟","时间同步","时间标准"],"title":"Linux硬件时钟与系统时钟","uri":"/posts/2020/06/02/linux-clock/"},{"categories":["系统工具"],"content":"系统时钟 系统时钟，又称软件时钟，是Linux内核的一部分，包括时间、时区、夏令时（如果适用）。 系统启动时，系统时钟的初始值根据硬件时钟设定（依赖于/etc/adjtime）； 系统启动后，Linux内核利用定时器中断维护系统时钟 ，与硬件时钟无关。 操作系统时钟的工具是timedatectl，用于查询/设置时间、设置时区、设置时间同步等。 查询时钟，其中包括本地时间、UTC时间、RTC时间、时区、系统时钟同步状态、NTP服务状态、RTC是否使用本地时间： # timedatectl Local time: Tue 2020-06-02 18:08:06 CST Universal time: Tue 2020-06-02 10:08:06 UTC RTC time: Tue 2020-06-02 10:08:06 Time zone: Asia/Shanghai (CST, +0800) System clock synchronized: yes NTP service: active RTC in local TZ: no 设置时钟，时间格式是yyyy-MM-dd hh:mm:ss： # timedatectl set-time \"2020-06-02 17:30:00\" # timedatectl Local time: Tue 2020-06-02 17:30:05 CST Universal time: Tue 2020-06-02 09:30:05 UTC RTC time: Tue 2020-06-02 09:30:05 Time zone: Asia/Shanghai (CST, +0800) System clock synchronized: no NTP service: inactive RTC in local TZ: no 时间字符串为本地时间，执行命令后，本地时间、UTC时间、RTC时间会同时更新。 开启时间同步： # timedatectl set-ntp 1 ","date":"2020-06-02","objectID":"/posts/2020/06/02/linux-clock/:2:0","tags":["Linux","硬件时钟","系统时钟","时间同步","时间标准"],"title":"Linux硬件时钟与系统时钟","uri":"/posts/2020/06/02/linux-clock/"},{"categories":["系统工具"],"content":"时间同步 可以使用ntpd -qg命令同步时间。 另外，Linux系统自带ntpd.service，可以使用systemctl start ntpd命令启动NTP服务。 需要注意的是，ntpd命令或服务仅更新本地时间和UTC时间，不更新RTC时间。 可以设置一个过去的时间，然后执行ntpd命令校准时间，再执行timedatectl命令查询验证。 不过这并不是什么大问题，因为关机时操作系统会根据系统时钟重设硬件时钟，下次启动时，二者恢复一致。 ","date":"2020-06-02","objectID":"/posts/2020/06/02/linux-clock/:3:0","tags":["Linux","硬件时钟","系统时钟","时间同步","时间标准"],"title":"Linux硬件时钟与系统时钟","uri":"/posts/2020/06/02/linux-clock/"},{"categories":["系统工具"],"content":"时间标准 两种时间标准： 本地时间（时区相关） UTC时间（时区无关） 硬件时钟所采用的时间标准是由操作系统决定的，默认情况下，Windows使用本地时间，macOS使用UTC时间，类Unix系统则各有不同。 使用UTC时间标准的操作系统通常把硬件时钟视为UTC，然后在启动时根据时区进行调整设置系统时间。 如果一台机器上安装了多个操作系统，它们都使用硬件时钟设置系统时间，建议采用统一的UTC时间标准，避免切换系统时发生时间错乱。 参考文档 https://wiki.archlinux.org/index.php/System_time ","date":"2020-06-02","objectID":"/posts/2020/06/02/linux-clock/:4:0","tags":["Linux","硬件时钟","系统时钟","时间同步","时间标准"],"title":"Linux硬件时钟与系统时钟","uri":"/posts/2020/06/02/linux-clock/"},{"categories":["编程框架"],"content":"DPDKData Plane Development Kit是Intel提供的数据平面开发套件，为用户空间高效的数据包处理提供驱动和函数库支持。 使用DPDK开发的应用程序，需要自行实现网络协议栈，使用门槛相对较高。 Seastar是一个面向现代硬件多核架构的高性能应用程序开发框架，它实现了native网络协议栈，同时集成DPDK。 最近使用DPDK对Seastar进行扩展，支持网卡bonding、Linux控制平面访问等，在此做一个小结。 ","date":"2020-04-07","objectID":"/posts/2020/04/07/seastar-dpdk/:0:0","tags":["Seastar","DPDK","网络协议栈"],"title":"Seastar和DPDK的开发总结","uri":"/posts/2020/04/07/seastar-dpdk/"},{"categories":["编程框架"],"content":"Linux控制平面访问 KNI（Kernel NIC Interface）允许DPDK应用程序访问Linux控制平面，比如使用SSH远程访问机器，KNI能够让ssh连接发送的数据到达sshd进程。 KNI有以下优势： 相比TUN/TAP接口，KNI避免两次内核态与用户态间的数据拷贝，更加高效 支持使用Linux标准网络工具（ifconfig/ethtool/tcpdump等）操作DPDK网络接口 支持在内核网络协议栈上创建网络接口 启用KNI，首先加载KNI内核模块rte_kni，该模块自动创建一个特殊设备/dev/kni，而后通过KNI的API函数调用与内核进行交互。 创建KNI虚拟网络接口，正确配置ip/netmask/boardcast/hwaddr/gateway，向KNI接口写入的数据可以顺利到达内核。 网卡和网关配置。必须与机器原有配置保持一致 ","date":"2020-04-07","objectID":"/posts/2020/04/07/seastar-dpdk/:1:0","tags":["Seastar","DPDK","网络协议栈"],"title":"Seastar和DPDK的开发总结","uri":"/posts/2020/04/07/seastar-dpdk/"},{"categories":["编程框架"],"content":"VLAN VLAN（Virtual LAN，虚拟局域网）用于局域网分割广播域，减少网络带宽和CPU消耗，提高网络处理能力。 VLAN环境下的机器需要配置vlan网络接口，接口命名规则是${interface}.${vlan_id}，vlan_id范围是0-4095。 以某物理机为例，eth0和eth1是物理网卡，bond0是在它们之上做的网卡bonding，bond0.107是bond0的vlan网络接口（107是vlan_id）。 [root@~]# lshw -businfo -class network Bus info Device Class Description ======================================================= pci@0000:06:00.0 eth0 network Ethernet Controller 10-Gigabit X540-AT2 pci@0000:06:00.1 eth1 network Ethernet Controller 10-Gigabit X540-AT2 bond0 network Ethernet interface bond0.107 network Ethernet interface 查看配置文件/etc/sysconfig/network-scripts/ifcfg-bond0.107，VLAN=\"yes\"表示其为vlan接口 IEEE 802.1Q标准封装的vlan数据帧在传统以太网数据帧上附加了vlan-tag，机器与与交换机的数据传输是否带vlan-tag，可以在机器上抓包分析；在此机器上测试，收到和发出的数据包均带有vlan-tag。 DPDK应用程序直接操作网卡队列，不需要vlan网络接口，收包时进行untagging（利用网卡自身offload功能将vlan-tag剥离至rte_mbuf），发包时进行tagging（为rte_mbuf设置vlan-tag）。 ","date":"2020-04-07","objectID":"/posts/2020/04/07/seastar-dpdk/:2:0","tags":["Seastar","DPDK","网络协议栈"],"title":"Seastar和DPDK的开发总结","uri":"/posts/2020/04/07/seastar-dpdk/"},{"categories":["编程框架"],"content":"RSS和多网卡队列 RSS（Receive Side Scaling）是一种网卡驱动技术，能够让网卡的数据流量分布在多个CPU上。 启用DPDK的RSS功能，需要预设Hash Function、Hash Key和Hash Types，其中Hash Function一般是Toeplitz哈希算法，Hash Types是一个标记位组合，标识哪类数据包接受RSS计算。 网卡收到数据包后，以数据包的IP端口等信息作为输入（根据packet类型，输入可能为二/三/四元组），经过Hash Function计算出一个哈希值，哈希值的LSBs（最低有效位）用于索引RETA（即indirection table），对应的值是网卡队列ID，即数据包将发往该网卡队列。 INTEL 82576/82599网卡的RETA是位宽为4的128项的索引映射表，取哈希值低7位（LSBs）作为索引；Seastar默认的RETA填充策略是按照网络队列ID从小到大依次填充，另外，RETA支持运行时动态调整；Seastar将每个网卡队列绑定到一个核，网卡队列ID即CPU ID。 Seastar应用程序与外部进行连接时，connect函数随机选择一个本地端口（local_port），然后对四元组(dst_ip, dst_port, local_ip, local_port)做RSS计算，如果结果与当前CPU ID一致，则使用该端口发起连接，目的是对端发送的数据由当前CPU接收和处理。 ","date":"2020-04-07","objectID":"/posts/2020/04/07/seastar-dpdk/:3:0","tags":["Seastar","DPDK","网络协议栈"],"title":"Seastar和DPDK的开发总结","uri":"/posts/2020/04/07/seastar-dpdk/"},{"categories":["编程框架"],"content":"数据包分发 Seastar的native协议栈实现了ARP/ICMP/TCP/UDP等协议，DPDK模式下收到的数据包全部进入native协议栈；在引入KNI后，需要对数据包进行解析，根据规则进入不同的协议栈。 ARP数据包 ARP数据包按类型分为request和Reply，request只需进入native协议栈；reply则需要知道request由哪个协议栈发起，进入该协议栈。 鉴于ARP请求处理开销不大，当前实现不管request还是reply，同时进入native协议栈和kernel协议栈。 ICMP数据包 ICMP请求的数据包进入native协议栈，当然也可以进入kernel协议栈，但需要经过KNI这一层的处理，会增加些许延迟。 同时进入两个协议栈，对端会出现ping (DUP!)错误 应用程序数据包 应用程序数据包根据目标端口区分，进入native协议栈，端口包括两类： server端口 - 用于accept客户端连接，在程序启动前已经确定 client端口 - connect函数调用所使用的本地端口，与CPU ID相关的随机端口 Seastar与外部建立连接后返回一个connected_socket，但无法确定所用的本地端口，当前实现是对应的dpdk_qp（相当于网卡队列）维护一个本地端口表，执行tcp::tcb::connect()建立连接前注册端口，执行tcp::tcb::cleanup()关闭连接后注销端口。 其他 上述三类之外的数据包，全部进入kernel协议栈。 ","date":"2020-04-07","objectID":"/posts/2020/04/07/seastar-dpdk/:4:0","tags":["Seastar","DPDK","网络协议栈"],"title":"Seastar和DPDK的开发总结","uri":"/posts/2020/04/07/seastar-dpdk/"},{"categories":["编程框架"],"content":"Seastar网络初始化 create_dpdk_net_device()函数创建DPDK设备： 初始化物理网卡（slave），根据网卡的物理特性启用offload，设置网卡队列长度，设置RSS 创建bond-port（名称是dpdk_bond，模式是8023AD），添加slave-port 初始化bond-port，配置读写队列数量、xmit_policy 创建kni（名字是dpdk_bond0） 创建rte_ring，用于多核通信（多写一读）的kni数据包队列 在每个core上，初始化对应的dpdk_qp，创建rx/tx队列 启动slave-ports和bond-port，设置reta，网卡均启动后，配置kni的IP/netmask/boardcast/hw/gateway 至此，网络初始化完成。 smp::configure(); engine().when_started().then([] {}); 参考资料 https://doc.dpdk.org/guides/prog_guide/kernel_nic_interface.html https://en.wikipedia.org/wiki/IEEE_802.1Q https://docs.microsoft.com/en-us/windows-hardware/drivers/network/introduction-to-receive-side-scaling https://docs.microsoft.com/en-us/windows-hardware/drivers/network/rss-hashing-functions https://docs.microsoft.com/en-us/windows-hardware/drivers/network/rss-hashing-types http://galsagie.github.io/2015/02/26/dpdk-tips-1/ ","date":"2020-04-07","objectID":"/posts/2020/04/07/seastar-dpdk/:5:0","tags":["Seastar","DPDK","网络协议栈"],"title":"Seastar和DPDK的开发总结","uri":"/posts/2020/04/07/seastar-dpdk/"},{"categories":["系统工具"],"content":"Software collections（SCLs）是一个Linux软件多版本共存的解决方案，适用于RHEL/CentOS/Fedora。 SCL不修改已安装的软件版本，也不会与其产生冲突。 CentOS是RHEL的社区版，服务器环境偏向于稳定安全，官方YUM源的软件版本相对保守，开发时经常遇到某些工具版本较低无法满足需求的问题，比如GCC、CMake等， 过去我一般是下载源码包编译安装，直到后来遇到SCL。 我的系统是CentOS 7.2，官方YUM源提供的GCC最高版本是4.8.5，下面演示如何安装并使用GCC 5。 首先安装scl工具和SCL软件源： yum install scl-utils yum install centos-release-scl 然后安装devtoolset-4： yum install devtoolset-4 SCL一般包含多个RPM包，devtoolset-4集合下软件包数量众多，有C相关的，有Java相关的，可以选择安装指定RPM包，如yum install devtoolset-4-gcc 此时系统已安装两个版本的GCC： GCC 4.8.5（官方YUM源） GCC 5.3.1（SCL） # which gcc /usr/bin/gcc # gcc --version gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-36) # scl enable devtoolset-4 -- which gcc /opt/rh/devtoolset-4/root/usr/bin/gcc # scl enable devtoolset-4 -- gcc --version gcc (GCC) 5.3.1 20160406 (Red Hat 5.3.1-6) scl命令用于激活SCL版本和在该环境下执行其他操作，可以同时激活多个SCL，执行其他命令的方式有： 直接运行应用程序 对于不带参数的命令 scl enable SCL_NAME_1 SCL_NAME_2 command 对于带参数的命令，有两种写法：引号和命令分隔符。 scl enable SCL_NAME_1 SCL_NAME_2 'command --argument' scl enable SCL_NAME_1 SCL_NAME_2 \"command --argument\" scl enable SCL_NAME_1 SCL_NAME_2 -- command --argument 新建一个Shell环境 在当前shell环境下新建一个shell并激活一个或多个SCL scl enable SCL_NAME_1 SCL_NAME_2 bash 在新的shell环境下，执行命令echo $X_SCLS，查看已激活的SCL； 退出环境，执行命令exit。 执行记录在文件中的命令 一般用于批量执行命令 cat cmd_file | scl enable SCL_NAME_1 SCL_NAME_2 - 查看已安装的SCL scl --list 查看指定SCL下已安装的RPM包 scl --list SCL_NAME 卸载整个SCL yum remove SCL_NAME\\* 参考 https://www.softwarecollections.org/ https://www.softwarecollections.org/en/docs/ ","date":"2020-02-06","objectID":"/posts/2020/02/06/scl/:0:0","tags":["Linux","CentOS","SCL"],"title":"SCL基本用法","uri":"/posts/2020/02/06/scl/"},{"categories":["系统工具"],"content":"安装debtap，这里用的是yay，使用其他AUR helper也一样。 yay -S debtap 使用debtap将deb包转换为arch包，假设deb包文件是demo.deb，转换生成一个名为demo-1-x86_64.pkg.tar.xz的arch包，最后使用pacman以离线方式进行安装。 debtap -q demo.deb sudo pacman -U demo-1-x86_64.pkg.tar.xz 参考 https://www.maketecheasier.com/how-to-install-deb-package-in-arch-linux/ https://wiki.archlinux.org/index.php/Offline_installation_of_packages https://en.wikipedia.org/wiki/Deb_(file_format) ","date":"2019-10-30","objectID":"/posts/2019/10/30/how-to-install-deb-package-in-manjaro/:0:0","tags":["Linux","Manjaro","deb"],"title":"如何在Manjaro安装deb包","uri":"/posts/2019/10/30/how-to-install-deb-package-in-manjaro/"},{"categories":["数据库"],"content":" 本文内容适用于MongoDB v3.2及以上版本 数据库分片 集合分片 实时同步原理 分片集群在config server存储集群的各种配置信息，比如节点地址、分片配置、数据分布等，可连接mongos切换到config数据库查询相关信息。 分片操作包括enableSharding和shardCollection命令，分别作用于数据库和集合，下面从这两个方面分别说明。 ","date":"2019-06-14","objectID":"/posts/2019/06/14/sharding-config-sync/:0:0","tags":["MongoDB","分片集群","数据同步"],"title":"MongoDB分片配置的实时同步","uri":"/posts/2019/06/14/sharding-config-sync/"},{"categories":["数据库"],"content":"数据库分片 enableSharding命令用于开启数据库的分片状态，只有当数据库开启分片后，其集合才能进一步分片。 config.database集合存储全部数据库的meta信息，例如： mongos\u003e use config switched to db config mongos\u003e db.databases.find() { \"_id\" : \"test\", \"primary\" : \"s1\", \"partitioned\" : true } { \"_id\" : \"test1\", \"primary\" : \"s2\", \"partitioned\" : false } { \"_id\" : \"test2\", \"primary\" : \"s4\", \"partitioned\" : true } { \"_id\" : \"test3\", \"primary\" : \"s6\", \"partitioned\" : true } 字段 说明 _id 数据库名称 primary 主shard partitioned 分片状态（true / false） 数据库的meta记录遵循以下规则： 创建数据库，对应的meta记录写入config.databases 删除数据库，对应的meta记录从config.databases删除 对于一个已存在但未开启分片的数据库执行分片操作，其meta记录的partitioned字段产生状态变化（false =\u003e true） ","date":"2019-06-14","objectID":"/posts/2019/06/14/sharding-config-sync/:1:0","tags":["MongoDB","分片集群","数据同步"],"title":"MongoDB分片配置的实时同步","uri":"/posts/2019/06/14/sharding-config-sync/"},{"categories":["数据库"],"content":"集合分片 shardCollection命令用于对集合分片。 config.collections仅存储分片集合的meta信息，例如： mongos\u003e use config switched to db config mongos\u003e db.collections.find() { \"_id\" : \"test.coll\", \"lastmodEpoch\" : ObjectId(\"5d03228754bfbe35e837a246\"), \"lastmod\" : ISODate(\"1970-02-19T17:02:47.301Z\"), \"dropped\" : false, \"key\" : { \"_id\" : \"hashed\" }, \"unique\" : false } { \"_id\" : \"test2.coll\", \"lastmodEpoch\" : ObjectId(\"5d03229c54bfbe35e837a24c\"), \"lastmod\" : ISODate(\"1970-02-19T17:02:47.296Z\"), \"dropped\" : false, \"key\" : { \"_id\" : 1 }, \"unique\" : true } { \"_id\" : \"test3.coll\", \"lastmodEpoch\" : ObjectId(\"000000000000000000000000\"), \"lastmod\" : ISODate(\"2019-06-14T04:31:49.622Z\"), \"dropped\" : true } 字段 说明 _id 集合namespace lastmodEpoch 更新时间点相关 lastmod 更新时间点相关 dropped 是否已删除（true / false） key 片键 unique 片键唯一性约束（true / false） 集合的meta记录遵循以下规则： 对一个新集合分片，对应的meta记录写入config.collections 删除一个分片集合，其meta记录的dropped字段产生状态变化（false =\u003e true，meta记录不会从config.collections删除) 对一个已经删除的分片集合重新分片，其meta记录的dropped字段产生状态变化（true =\u003e false，此时meta记录已经存在） ","date":"2019-06-14","objectID":"/posts/2019/06/14/sharding-config-sync/:2:0","tags":["MongoDB","分片集群","数据同步"],"title":"MongoDB分片配置的实时同步","uri":"/posts/2019/06/14/sharding-config-sync/"},{"categories":["数据库"],"content":"实时同步原理 从MongoDB 3.2版本起，分片集群config server支持复制集，为实时同步提供了基础。 分片配置同步的原理与数据同步类似，监听并回放config server的oplog，只是回放逻辑需要依据以下规则： {ns: ‘config.databases’, op: ’i‘， partitioned: false} 新建数据库 {ns: 'config.databases', op: 'i'， partitioned: true} 新建数据库并执行分片 {ns: 'config.databases', op: 'd'} 删除数据库 {ns: 'config.databases', op: 'u'} 对数据库执行分片，此oplog表示对未分片的数据库执行分片 {ns: 'config.collections', op: 'i'} 新建集合并执行分片 {ns: 'config.collections', op: 'u'， dorpped: false} 对集合执行分片，此oplog表示对一个已经删除的分片集合重新分片 {ns: 'config.collections', op: 'u'， dorpped: true} 删除集合 ","date":"2019-06-14","objectID":"/posts/2019/06/14/sharding-config-sync/:3:0","tags":["MongoDB","分片集群","数据同步"],"title":"MongoDB分片配置的实时同步","uri":"/posts/2019/06/14/sharding-config-sync/"},{"categories":["系统工具"],"content":"Thunderbird是一款好用的Linux邮件客户端。 ","date":"2019-04-29","objectID":"/posts/2019/04/29/thunderbird/:0:0","tags":["Linux","Thunderbird"],"title":"Thunderbird邮件客户端配置","uri":"/posts/2019/04/29/thunderbird/"},{"categories":["系统工具"],"content":"账户 添加账户，窗口左侧Local Folders 右键选择 settings 进入account settings，左下角 account actions 有账户的添加/删除等操作，然后配置接收/发送邮件服务器就ok了。 ","date":"2019-04-29","objectID":"/posts/2019/04/29/thunderbird/:1:0","tags":["Linux","Thunderbird"],"title":"Thunderbird邮件客户端配置","uri":"/posts/2019/04/29/thunderbird/"},{"categories":["系统工具"],"content":"同步\u0026存储 默认同步所有邮件。 在account settings下点击目标账户的 Synchronization\u0026Strorage 标签，在 Disk Space 下设置好要同步的邮件，支持同步最近一段时间的邮件。 ","date":"2019-04-29","objectID":"/posts/2019/04/29/thunderbird/:2:0","tags":["Linux","Thunderbird"],"title":"Thunderbird邮件客户端配置","uri":"/posts/2019/04/29/thunderbird/"},{"categories":["系统工具"],"content":"订阅邮件目录 右键邮件账户，点击 Subcribe，选择需要同步的远程邮件目录。 ","date":"2019-04-29","objectID":"/posts/2019/04/29/thunderbird/:3:0","tags":["Linux","Thunderbird"],"title":"Thunderbird邮件客户端配置","uri":"/posts/2019/04/29/thunderbird/"},{"categories":["系统工具"],"content":"邮件操作 对于“已发送/已删除/草稿”邮件，Thunderbird 默认将其存放在远程邮件账户的Trash/Sent/Drafts，如果远程邮件账户下没有相应的同名目录，则会自动创建；如果邮件服务器使用中文命名的邮件目录，所以需要重新设置。 公司使用Outlook邮件服务器，所以我在Server Settings下 将 Trash 替换为 已删除的邮件，Copies \u0026 Folders下 将 Sent 替换为 已发送邮件，Drafts 替换为 草稿。 关于“已发送/已删除/草稿”邮件的存放位置，我是遵循Outlook Web App的操作逻辑，目标位置都设置的远程目录，然后同步到本地。 Server Settings下新邮件检查间隔，删除操作（移动到指定邮件目录）。 ","date":"2019-04-29","objectID":"/posts/2019/04/29/thunderbird/:4:0","tags":["Linux","Thunderbird"],"title":"Thunderbird邮件客户端配置","uri":"/posts/2019/04/29/thunderbird/"},{"categories":["系统工具"],"content":"发送服务器 左侧OutgoingServer(SMTP)可以管理不同的smtp，目标账户下可以配置为其选择smtp。 ","date":"2019-04-29","objectID":"/posts/2019/04/29/thunderbird/:5:0","tags":["Linux","Thunderbird"],"title":"Thunderbird邮件客户端配置","uri":"/posts/2019/04/29/thunderbird/"},{"categories":["系统工具"],"content":"邮件签名 account settings默认tab下的 Signature text 可以配置签名。 ","date":"2019-04-29","objectID":"/posts/2019/04/29/thunderbird/:6:0","tags":["Linux","Thunderbird"],"title":"Thunderbird邮件客户端配置","uri":"/posts/2019/04/29/thunderbird/"},{"categories":["系统工具"],"content":"用户名/密码管理 account settings默认tab下的 Manage Identities 可以管理用户名/密码。 ","date":"2019-04-29","objectID":"/posts/2019/04/29/thunderbird/:7:0","tags":["Linux","Thunderbird"],"title":"Thunderbird邮件客户端配置","uri":"/posts/2019/04/29/thunderbird/"},{"categories":["系统工具"],"content":"平时使用SecureCRT远程连接到CentOS开发机，Vim用了jellybeans配色方案，配置如下： set t_Co=256 colorscheme jellybeans bash下一切正常，但在tmux下Vim配色方案有问题，查了下，tmux -2可以解决。 看了下tmux的man手册，-2选项让tmux假设终端支持256种颜色，而jellybeans配色方案也是基于256色，二者一致，所以不存在兼容问题。 -2 Force tmux to assume the terminal supports 256 colours. 可以在~/.bashrc下添加一行： alias tmux='tmux -2' ","date":"2018-07-24","objectID":"/posts/2018/07/24/tmux-vim-color/:0:0","tags":["Vim","tmux"],"title":"Vim在tmux下的配色方案不兼容问题","uri":"/posts/2018/07/24/tmux-vim-color/"},{"categories":["数据库"],"content":"直接从问题出发，数据备份与恢复相关的。 备份，从源数据库导出全量数据（mongodump），备份oplog； 恢复，向目标数据库导入全量数据（mongorestore），重放oplog。 正常来讲，源端和目标端数据库最终数据一致，可以通过dbHash命令校验，但实际情况是dbHash校验结果不一致。 排查发现，时间点固定，多次执行数据恢复（删除数据-全量导入-重放oplog），每次目标数据库的dbHash结果都不一样，可以确定问题出在重放oplog这里。 既然重放过程中程序并没有异常，说明数据成功写入，原因只应该是数据的二进制存储内容不一致。 程序用Go写的，mongo驱动是mgo，oplog读出来后用Bson.M暂存，然后将其Marshal并写入文件，Bson.M的API文档描述如下： type M map[string]interface{} M is a convenient alias for a map[string]interface{} map, useful for dealing with BSON in a native way. For instance: bson.M{\"a\": 1, \"b\": true} There’s no special handling for this type in addition to what’s done anyway for an equivalent map type. Elements in the map will be dumped in an undefined ordered. See also the bson.D type for an ordered alternative. 再看看Bson.D： type D []DocElem D represents a BSON document containing ordered elements. For example: bson.D{ {\"a\", 1}, {\"b\", true} } In some situations, such as when creating indexes for MongoDB, the order in which the elements are defined is important. If the order is not important, using a map is generally more comfortable. See bson.M and bson.RawD. 这里解释下BSON，Binary JSON的缩写，JSON的二进制序列，MongoDB底层采用的数据存储格式。 Bson.M是map类型，内部元素在导出时顺序是不确定的，比如说{a: 1, b: 2}，可能先导出a，也可能先导出b，两份bson数据逻辑上相同，二进制存储内容不同，所以md5算出来的结果也不同； Bson.D是array类型，元素有序，反序列化过程逐步追加到数组末尾，而且提到有些情况下数据顺序很关键，比如建索引，在我之前的一个文章里也提到。 好，这里增加一个情况，基于oplog重放的数据恢复与校验。 ","date":"2018-05-24","objectID":"/posts/2018/05/24/mongodb-document-element-order/:0:0","tags":["MongoDB"],"title":"MongoDB文档数据顺序","uri":"/posts/2018/05/24/mongodb-document-element-order/"},{"categories":["系统工具"],"content":"Last updated: 2019-06-14 Last updated: 2019-04-29 以下摘自维基百科： Manjaro Linux（或简称Manjaro）是基于Arch Linux的Linux发行版，使用Xfce和KDE Plasma作为默认桌面环境，和Arch一样，采用滚动更新。其目标是为PC提供易于使用的自由的操作系统。 Manjaro Linux基于Arch Linux，但拥有自己独立的软件仓库。Manjaro的目标是让强大的Arch更方便用户使用，Manjaro使用著名的Pacman且可以直接利用AUR上的资源。Manjaro本身使用三个软件仓库：不稳定库，即含有那些不成熟的Arch包，这些包与Arch源有1-2天的延后；测试库，每周同步一次，包含那些Arch不稳定源的包；以及稳定库，包含那些由开发团队确认稳定的软件。 体验过几种不同的DE（Desktop Environment），包括Xfce、GNOME 3、Cinnamon、Deepin，感觉传统桌面更适合自己。 自用低配笔记本是Xfce，公司高配笔记本最初是Cinnamon，整体感觉不错，后来在Win7下删除win分区导致Linux无法引导，折腾一个下午未能修复，最后重装Manjaro，顺便换车过了Xfce。 用了一段时间，目前已是Xfce忠实粉丝，原生桌面视觉上比较单一，但可配置性很高，需要什么自己折腾就好了。 ","date":"2018-02-20","objectID":"/posts/2018/02/20/install-manjaro/:0:0","tags":["Manjaro","Linux"],"title":"Manjaro安装与配置","uri":"/posts/2018/02/20/install-manjaro/"},{"categories":["系统工具"],"content":"安装 下载ISO镜像，使用Rufus制作U盘安装盘，选择DD模式。 ","date":"2018-02-20","objectID":"/posts/2018/02/20/install-manjaro/:0:1","tags":["Manjaro","Linux"],"title":"Manjaro安装与配置","uri":"/posts/2018/02/20/install-manjaro/"},{"categories":["系统工具"],"content":"配置源和更新系统 包管理工具配置国内源，速度较快。 sudo pacman-mirrors -i -c China -m rank 在交互窗口里勾选目标源（可多选），刷新缓存和更新系统。 sudo pacman -Syyu ","date":"2018-02-20","objectID":"/posts/2018/02/20/install-manjaro/:0:2","tags":["Manjaro","Linux"],"title":"Manjaro安装与配置","uri":"/posts/2018/02/20/install-manjaro/"},{"categories":["系统工具"],"content":"安装中文输入法 Fcitx (Flexible Input Method Framework) ──即小企鹅输入法，它是一个以 GPL 方式发布的输入法平台,可以通过安装引擎支持多种输入法，支持简入繁出，是在 Linux 操作系统中常用的中文输入法。它的优点是，短小精悍、跟程序的兼容性比较好。 如果是Xfce或Cinnamon桌面，运行下面命令；其他DE仅供参考，比如GNOME 3默认是ibus输入法框架，二者冲突。 sudo pacman -S fcitx fcitx-im fcitx-configtool fcitx-googlepinyin 此处建议直接安装fcitx-im，默认包含全部模块（fcitx-gtk2/fcitx-gtk3/fcitx-qt4/fcitx-qt5），如果仅安装fcitx-gtk，那么使用QT开发的桌面工具可能无法调出输入法。 编辑 ~/.xprofile，添加配置： export GTK_IM_MODULE=fcitx export QT_IM_MODULE=fcitx export XMODIFIERS=\"@im=fcitx\" 安装完毕，注销或重启系统，输入法即生效。 ","date":"2018-02-20","objectID":"/posts/2018/02/20/install-manjaro/:0:3","tags":["Manjaro","Linux"],"title":"Manjaro安装与配置","uri":"/posts/2018/02/20/install-manjaro/"},{"categories":["系统工具"],"content":"安装其他软件 yaourt已停止开发，现已无法通过pacman安装 用于安装AUR上的包，AUR资源非常丰富，很多官方仓库没有的，可以在AUR找到，比如vscode、搜狗输入法。 Arch用户软件仓库（Arch User Repository，AUR）是为用户而建、由用户主导的Arch软件仓库。AUR中的软件包以软件包生成脚本（PKGBUILD）的形式提供，用户自己通过makepkg生成包，再由pacman安装。创建AUR的初衷是方便用户维护和分享新软件包，并由官方定期从中挑选软件包进入community仓库。 许多官方仓库软件包都来自AUR。通过AUR，大家相互分享新的软件包生成脚本（PKGBUILD和其他相关文件）。用户还可以为软件包投票。如果一个软件包投票足够多、没有协议问题、打包质量好，那么它就很有希望被收录进官方[community]仓库（以后就可以直接通过pacman 或 abs 安装了）。 sudo pacman -S yaourt yay 替代yaourt sudo pacman -S yay Chrome浏览器 sudo pacman -S chromium vscode yay -S visual-studio-code-bin 截图工具deepin-screenshot sudo pacman -S deepin-screenshot 好了，新年到了，新年快乐！ 参考资料 https://zh.wikipedia.org/wiki/Manjaro_Linux#cite_note-31 https://mirrors.ustc.edu.cn/help/manjaro.html https://wiki.archlinux.org/index.php/Fcitx_(%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87) https://wiki.archlinux.org/index.php/Arch_User_Repository_(%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87)#.E6.9E.84.E5.BB.BA.E5.92.8C.E5.AE.89.E8.A3.85.E8.BD.AF.E4.BB.B6.E5.8C.85 ","date":"2018-02-20","objectID":"/posts/2018/02/20/install-manjaro/:0:4","tags":["Manjaro","Linux"],"title":"Manjaro安装与配置","uri":"/posts/2018/02/20/install-manjaro/"},{"categories":["系统工具"],"content":"Git的rebase命令用于操作commits，包括合并、删除、提交内容修改、注释修改等。 比如实现了一个新功能，开发过程中提了多个commit，其中也有bugix，在正式push前，需要对commits进行精简，这时rebase命令就派上用场了。 我使用的Git版本是1.7.1，rebase的commit操作命令如下表，更高版本的Git提供更多的命令。 命令(,简写) 说明 pick, p use commit reword, r use commit, but edit the commit message edit, e use commit, but stop for amending squash, s use commit, but meld into previous commit fixup, f like “squash”, but discard this commit’s log message 其中， pick 保留commit reword 保留commit，但重新编辑该commit的注释 edit 保留commit，但rebase操作执行到该commit时会暂停，用户在此commit基础上进行内容修改（当然，也可以不修改）后，执行git add 目标文件和git commit --amend更新注释，然后执行git rebase --continue继续未完成的rebase工作。 这个命令的使用场景是，需要对之前的某一个commit上做内容修改。 squash 保留commit，但将其合并至前一个commit，合并时，自动汇总该commit与上一个commit的注释，用户可进行二次编辑 fixup 与squash功能类似，保留commit，将其合并至前一个commit，但是丢弃该commit的注释信息 ","date":"2018-02-12","objectID":"/posts/2018/02/12/git-rebase/:0:0","tags":["Git"],"title":"Git rebase命令","uri":"/posts/2018/02/12/git-rebase/"},{"categories":["系统工具"],"content":"Linux下使用Vim配置Python开发环境，我能想到的，首先要有语法高亮，然后代码自动补全，提高编码效率，再就是语法错误检查，便于纠错，最后是遵循编码规范，适合分享代码。 下面说说一些必要的Vim插件。 语法高亮 安装python.vim插件，同时~/.vimrc包含以下配置： syntax on filetype plugin on 代码自动补全 安装jedi-vim插件，其依赖于Python自动补全工具Jedi，所以要在本机安装Jedi。 pip install jedi 语法错误和编码规范检查 这两个放在一起说，是因为vim-flake8插件同时搞定这两件事，其依赖于flake8工具。 pip install flake8 flake8是集成了pyflakes和pycodestyle，其中，pyflakes检查代码错误；pycodestyle检查代码是否遵循Python编码规范（PEP8），其取代了pep8，后者已经不再更新。 安装完毕，使用Vim打开.py文件，按F7进行代码检查。 如果是用Vundle管理插件，相关配置如下： Plugin 'python.vim' Plugin 'davidhalter/jedi-vim' Plugin 'nvie/vim-flake8' 参考资料 https://pypi.python.org/pypi/flake8 https://pypi.python.org/pypi/pyflakes https://pypi.python.org/pypi/pycodestyle/ https://pypi.python.org/pypi/pep8/1.7.1 https://www.python.org/dev/peps/pep-0008/ ","date":"2017-12-24","objectID":"/posts/2017/12/24/python-vim/:0:0","tags":["Python","Vim","Linux"],"title":"Linux下使用Vim配置Python开发环境","uri":"/posts/2017/12/24/python-vim/"},{"categories":["系统工具"],"content":"Prometheus是一个基于时间序列存储的开源监控\u0026报警系统，提供原生Web页面，支持数据的查询/聚合/曲线生成，metrics数据呈现一般接入Grafana。 ","date":"2017-10-27","objectID":"/posts/2017/10/27/prometheus/:0:0","tags":["Prometheus","监控系统"],"title":"Prometheus入门教程","uri":"/posts/2017/10/27/prometheus/"},{"categories":["系统工具"],"content":"特点 多维数据模型，时间序列由metric名字和K/V标签标识 灵活的查询语言(PromQL) 单机模式，不依赖分布式存储 基于HTTP采用pull方式收集数据 支持push数据到中间件(pushgateway) 通过服务发现或静态配置发现目标 多种图表和仪表盘 ","date":"2017-10-27","objectID":"/posts/2017/10/27/prometheus/:1:0","tags":["Prometheus","监控系统"],"title":"Prometheus入门教程","uri":"/posts/2017/10/27/prometheus/"},{"categories":["系统工具"],"content":"组件 Prometheus生态系统由多个组件构成，其中多是可选的，根据具体情况选择 Prometheus server - 收集和存储时间序列数据 client library - 用于client访问server/pushgateway pushgateway - 对于短暂运行的任务，负责接收和缓存时间序列数据，同时也是一个数据源 exporter - 各种专用exporter，面向硬件、存储、数据库、HTTP服务等 alertmanager - 处理报警 其他各种支持的工具 ","date":"2017-10-27","objectID":"/posts/2017/10/27/prometheus/:2:0","tags":["Prometheus","监控系统"],"title":"Prometheus入门教程","uri":"/posts/2017/10/27/prometheus/"},{"categories":["系统工具"],"content":"基本概念 ","date":"2017-10-27","objectID":"/posts/2017/10/27/prometheus/:3:0","tags":["Prometheus","监控系统"],"title":"Prometheus入门教程","uri":"/posts/2017/10/27/prometheus/"},{"categories":["系统工具"],"content":"数据模型 metric名字和标签集合确定一个唯一的时间序列；时间序列数据样本由一个毫秒精度的时间戳和一个float64类型的值组成。 例如，一个名为http_request的metric，标签有method/host/uri等，表现此metric的多个维度，可表示为http_request{method: “”, host: “”, uri: “”, …}，那么不同的标签组合将确定不同的时间序列。 http_request{method: \"POST\"} 请求类型为POST的时间序列 http_request{host: \"host-1\"} 目标主机为host-1的时间序列 http_request{host: \"host-2\", uri: \"/index.htm\"} 目标主机为host-2同时uri为index.htm的时间序列 http_request{method: \"GET\", host: \"host-3\"} 请求类型是GET同时目标主机为host-3的时间序列 ","date":"2017-10-27","objectID":"/posts/2017/10/27/prometheus/:3:1","tags":["Prometheus","监控系统"],"title":"Prometheus入门教程","uri":"/posts/2017/10/27/prometheus/"},{"categories":["系统工具"],"content":"metric类型 client libraries提供了四种metric类型，包括Counter、Gauge、Histogram、Summary。 Counter 计数器，只允许增加或重置为0，不允许减少，比如服务的请求数。Counter支持用rate()函数计算平均值，比如QPS。建议使用 _total 作为后缀命名。 Gauge 非固定的值，比如CPU负载 、内存使用量。 作者起初没有充分理解时间序列的概念，试图使用Gauge记录请求的响应时间，后来发现这是个错误。 Gauge实际是一个值，其变化取决于server是否采集了数据，衡量的是一个事物的状态变化，比如内存使用量，内存始终是那个内存，只是其使用量会发生变化。 回到请求响应时间的场景，因为请求不是固定的，一秒内可能处理几千个请求，即使每个请求的响应时间都记录到Gauge，server采集到的也只是采集时刻前最后一个请求的响应时间，而这个值无法代表采集间隔期间其他请求的响应时间，没有任何说服力。 Histogram 采样观测值，可进行分位计算和数据聚合，计算在server端完成。 一个名为\u003cbasename\u003e的metric，其histogram有3个固定的时间序列 \u003cbasename\u003e_bucket 不同bucket下的观测值的累加数量 \u003cbasename\u003e_sum 观测值的总和 \u003cbasename\u003e_count 观测值的数量 Summary 采样观测值，与histogram不同的是，数量/总和/分位的计算在client端完成，计算结果存在server。因为没有最初的metric数据，所以summary不支持数据聚合。 一个名为\u003cbasename\u003e的metric，其summary有3个固定的时间序列 \u003cbasename\u003e{quantile=\"\u003cφ\u003e\"} \u003cbasename\u003e_sum 观测值的总和 \u003cbasename\u003e_count 观测值的数量 ","date":"2017-10-27","objectID":"/posts/2017/10/27/prometheus/:3:2","tags":["Prometheus","监控系统"],"title":"Prometheus入门教程","uri":"/posts/2017/10/27/prometheus/"},{"categories":["系统工具"],"content":"关于job和instance标签 instance是指收集数据的目标端点，一般对应于一个进程；而job表示实现同一功能或目标的一组instance。 Prometheus采集到数据后自动为其附加job和instance标签，其中job由Prometheus配置文件定义，instance是目标数据源的地址\u003chost\u003e:\u003cport\u003e。 ","date":"2017-10-27","objectID":"/posts/2017/10/27/prometheus/:3:3","tags":["Prometheus","监控系统"],"title":"Prometheus入门教程","uri":"/posts/2017/10/27/prometheus/"},{"categories":["系统工具"],"content":"查询 Prometheust提供函数表达式语言，用户可以实时地查询和聚合时间序列数据，表达式计算结果可以曲线图(Prometheus web graph)和表格(Prometheus web console)的形式呈现，也通过HTTP API由外部系统消费。 ","date":"2017-10-27","objectID":"/posts/2017/10/27/prometheus/:4:0","tags":["Prometheus","监控系统"],"title":"Prometheus入门教程","uri":"/posts/2017/10/27/prometheus/"},{"categories":["系统工具"],"content":"表达式语言数据类型 Prometheus表达式语言中，一个表达式或子表达式的计算结果是下列四个类型之一： instant vector 即时向量，位于同一时间点的一组时间序列，每个时间序列具有一个采样 range vector 范围向量，位于同一时间段的一组时间序列，每个时间序列具有一系列采样 scalar 标量，一个浮点数值 string 字符串，一个字符串值 不同的使用场景，需要特定类型的表达式，例如，只有返回instant vector的表达式可以直接用于绘图。 ","date":"2017-10-27","objectID":"/posts/2017/10/27/prometheus/:4:1","tags":["Prometheus","监控系统"],"title":"Prometheus入门教程","uri":"/posts/2017/10/27/prometheus/"},{"categories":["系统工具"],"content":"时间序列选择器 instant vector selectors 即时向量选择器允许选择一组时间序列，每个时间序列在目标时间点都有一个采样，最简单的形式是指定一个metric名字。 例如，返回metric名为http_requests_total的全部时间序列： http_requests_total 可以用{}附加标签对时间序列进行过滤。 例如，返回metric名为http_requests_total、job是prometheus、group是canary的全部时间序列： http_requests_total{job=\"prometheus\",group=\"canary\"} 也支持正则表达式： = 等于 != 不等于 =~ 满足正则匹配 !~ 不满足正则匹配 例如，返回metric名为http_requests_total、environment为staging/testing/development其中之一、method不是GET的全部时间序列。 http_requests_total{environment=~\"staging|testing|development\",method!=\"GET\"} 标签匹配可以不给定metric名字，返回的是具有此标签值的全部时间序列；如果标签匹配目标是一个空值（空字符串），返回不包含此标签的全部时间序列。 向量选择器规定至少要有一个标签的匹配目标不是空值。 {job=~\".*\"} # 错误，可能是空值 {job=~\".+\"} # 正确，非空值 {job=~\".*\",method=\"get\"} # 正确，job可能是空值，method是非空值 标签匹配器可使用内部标签__name__实现metric名字匹配。表达式http_requests_total等同于{__name__=\"http_requests_total\"}。 其他匹配符也可使用，例如，下面表达式返回所有名字以http_开头的metrics。 {__name__=~\"^http_.*\"} range vector selectors 范围向量选择器与即时向量选择器工作原理相同，只不过返回当前时间以前的一系列采样。 时间范围由附加在向量选择器尾部的[]指定，具体的值由数字和单位组成，时间单位包括： s - 秒 m - 分钟 h - 小时 d - 天 w - 星期 y - 年 例如，返回5分钟内metric名为http_requests_total、job是prometheus的全部时间序列。 http_requests_total{job=\"prometheus\"}[5m] offset modifier offset修饰符允许在单个即时向量或范围向量查询中设置相对于当前时间的时间偏移。 下面的表达式返回http_requests_total5分钟前的值： http_requests_total offset 5m 语法上offset修饰符应紧跟在selector后面，例如： sum(http_requests_total{method=\"GET\"} offset 5m) //正确 sum(http_requests_total{method=\"GET\"}) offset 5m //错误 同样适用于范围向量查询，下面表达式返回http_requests_total一周前的5分钟的QPS： rate(http_requests_total[5m] offset 1w) ","date":"2017-10-27","objectID":"/posts/2017/10/27/prometheus/:4:2","tags":["Prometheus","监控系统"],"title":"Prometheus入门教程","uri":"/posts/2017/10/27/prometheus/"},{"categories":["系统工具"],"content":"函数 内置函数很多，说几个使用过的，其他函数可参考https://prometheus.io/docs/prometheus/latest/querying/functions/ 函数 功能 rate 计算每秒平均值，仅适用于Counter，例如统计QPS sum 计算求和 histogram_quantile 计算分位值 ","date":"2017-10-27","objectID":"/posts/2017/10/27/prometheus/:5:0","tags":["Prometheus","监控系统"],"title":"Prometheus入门教程","uri":"/posts/2017/10/27/prometheus/"},{"categories":["系统工具"],"content":"注意事项 时间对齐 TODO 数据过期 如果一个时间序列超过5分钟没有采集到数据，即为过期，具体表现为该时间序列从图中消失。 避免慢查询和过载 如果数据量很大，可能导致绘图超时和server/browser过载。在未知数据上构建查询，建议先在Web Console下调试，确保结果合理（最多有几百个时间序列）。数据经过充分地过滤或聚合后，再切换到Web Graph。如果还是超时，可使用recoding rule预先生成数据。 ","date":"2017-10-27","objectID":"/posts/2017/10/27/prometheus/:6:0","tags":["Prometheus","监控系统"],"title":"Prometheus入门教程","uri":"/posts/2017/10/27/prometheus/"},{"categories":["随笔"],"content":"不多说什么了，看图 ","date":"2017-10-24","objectID":"/posts/2017/10/24/1024/:0:0","tags":null,"title":"1024随笔","uri":"/posts/2017/10/24/1024/"},{"categories":["编程语言"],"content":"本书以面试两个候选人的形式，对C\u0026C++深层知识展开讨论，颇为新颖。 C99和C++98标准中，程序退出码（exit value）用于表示程序是否成功执行，更早的ANSI C和K\u0026R标准，退出码没有任何意义 C标准中void表示函数没有任何参数，比如int main(void)表示main函数无参数，而int main()表示main函数可以接收任意多个参数 C标准规定源文件以空行结束 C标准规定static变量初始化为0，auto变量不进行初始化；目的是确保运行效率，static变量位于全局存储区，初始化操作仅在程序启动时执行一次memset操作 C++稍有不同，static变量初始化为其类型默认值，而非直接设置为0，当然内置类型的默认值是0 序列点是程序执行序列中的一个点，其前的副作用已经发生，其后的副作用尚未发生，两点之间一个对象的值通过执行语句至多能修改一次（a = a++产生未定义的值），并且先前的值是只读的（a + a++产生未定义的值） （未完待续） ","date":"2017-10-24","objectID":"/posts/2017/10/24/deep-c-and-c++/:0:0","tags":["C","C++"],"title":"《深入理解C\u0026C++》读书笔记","uri":"/posts/2017/10/24/deep-c-and-c++/"},{"categories":["编程语言"],"content":"比如一个产品在某个版本之后新增了某个特性，那么在用目标特性时，版本比较是必需的。 假设版本号由数字和点符号构成，格式为a.b.c，那么在Python下可以轻松实现。 下面是在IPython下的示例： In [1]: v1 = '3.4.6' In [2]: t1 = tuple(int(val) for val in v1.split('.')) In [3]: t1 Out[3]: (3, 4, 6) In [4]: v2 = '3.2.0' In [5]: t2 = tuple(int(val) for val in v2.split('.')) In [6]: t2 Out[6]: (3, 2, 0) In [7]: t1 \u003e t2 Out[7]: True 将版本号的数字解析为一个元组，利用元组的原生比较，判断版本的高低。 元组或列表的原生比较规则是逐位比较： 如果元素类型相同，进行比较，返回结果 如果元素类型不同，检查二者是否为数字 如果均是数字，执行必要的强制数字类型转换，然后比较（个人理解是不同的数字类型，比如int和float） 如果其中一个元素是数字，则另一个元素大，（数字在类型上是最小的） 否则，按类型名字的字母表顺序进行比较 如果其中一个元组先到达末尾，则长度更长的元组为更大的一方；如果元组长度相同，对应位置的元素也相同，那么两个元组相等。 参考 https://www.tutorialspoint.com/python/tuple_cmp.htm https://www.tutorialspoint.com/python/list_cmp.htm ","date":"2017-09-14","objectID":"/posts/2017/09/14/compare-version-in-python/:0:0","tags":["Python"],"title":"Python下实现版本号比较","uri":"/posts/2017/09/14/compare-version-in-python/"},{"categories":["数据库"],"content":"有网友反馈py-mongo-sync同步异常，检查发现curosr[0]取查询结果第一个文档时报错\"no such item for Cursor instance”。 这里的逻辑是，根据timestamp查询oplog起始位置，cursor类型是TAILABLE，然后取出第一条oplog，验证timestamp是否一致。 对方mongo版本是v3.4，同步工具在v3.2及更早版本的MongoDB上还没出过类似问题。 使用pymongo 3.5.1（最新版本）分别用v3.2和v3.4跑同步测试，在query结果非空的情况下，v3.2正常，v3.4报错，怀疑是不是v3.4做了什么改动，导致dirver不兼容。 进一步排查，cursor[0]读取文档，db端返回了一个错误，意思说tailable和singleBatch两个选项冲突。 {'number_returned': 1, 'data': [SON([(u'ok', 0.0), (u'errmsg', u\"cannot use tailable option with the 'singleBatch' option\"), (u'code', 2), (u'codeName', u'BadValue')])], 'starting_from': 0, 'cursor_id': 0} cursor.__getitem__(self, index)方法，如果取单个文档，首先对当前cursor进行clone，将limit设置为-1（db返回一条文档并关闭cursor，可参考ntoreturn vs batchSize），表示只读取一条文档。 577 if isinstance(index, integer_types): 578 if index \u003c 0: 579 raise IndexError(\"Cursor instances do not support negative \" 580 \"indices\") 581 clone = self.clone() 582 clone.skip(index + self.__skip) 583 clone.limit(-1) # use a hard limit 584 for doc in clone: 585 return doc 586 raise IndexError(\"no such item for Cursor instance\") 587 raise TypeError(\"index %rcannot be applied to Cursor \" 588 \"instances\" % index) cursor类型是TAILABLE或TAILABLE_AWAIT，所以query的tailable选项为True； cursor.limit设置为-1，导致query的singleBatch选项为True； 二者皆为True，引发冲突。 因为只读取一条文档，limit必须为-1，所以需要把TAILABLE和TAILABLE_AWAIT设置为0，避免选项冲突。 clone.limit(-1) # use a hard limit clone.__query_flags \u0026= ~CursorType.TAILABLE_AWAIT # PYTHON-1371 至此，问题解决，说说几点收获： 尽量不要使用cursor索引访问文档，除非是一次性操作 # 以下是原代码逻辑 coll = self._src_mc['local'].get_collection('oplog.rs', codec_options=bson.codec_options.CodecOptions(document_class=bson.son.SON)) cursor = coll.find({'ts': {'$gte': oplog_start}}, cursor_type=pymongo.cursor.CursorType.TAILABLE_AWAIT, no_cursor_timeout=True) if cursor[0]['ts'] == oplog_start： # 这里对原始cursor进行clone，执行查询（第一次），然后关闭cursor_cloned # 之前在跑同步时，时间戳一致，但此处可能仍有时间长短不一的等待，始终不明所以 # 原因是下面在调用cursor.next()时会重新执行查询 while True: oplog = cursor.next() # 这里使用原始cursor，执行查询（第二次） # handle oplog 提交PR前，确保本地跑通test case，当时仅考虑到TAILABLE的情况，而忽略了TAILABLE_AWAIT 掌握PR流程：提交PR后，如果review未通过，需要修改代码，在你的local分支下继续commit并push到GitHub，新commit会自动追加到该PR，最后由项目维护者完成merge，可以参考collaborating-with-issues-and-pull-requests 最后，很高兴PR能被官方merge :) ","date":"2017-09-12","objectID":"/posts/2017/09/12/pymongo-bugfix/:0:0","tags":["MongoDB","pymongo"],"title":"pymongo bugfix后记","uri":"/posts/2017/09/12/pymongo-bugfix/"},{"categories":["系统工具"],"content":"是否经历过这样的场景？ 比如，需要按某种方式去处理一个文本，然后去查有没有现成的Linux命令。果然有！好吧，我把你记录下来。 当然，“不起眼”是相对我个人来说的，可能在大家看来这都是比较常见的命令，那必须是我的孤陋寡闻了，见笑。 ","date":"2017-07-27","objectID":"/posts/2017/07/27/linux-magic-commands/:0:0","tags":["Linux"],"title":"一些不起眼却很实用的Linux命令（不定期更新）","uri":"/posts/2017/07/27/linux-magic-commands/"},{"categories":["系统工具"],"content":"文件/目录操作 realpath 获取目标文件或目录的绝对路径。 mktemp 用于创建一个临时的文件或目录。 默认在/tmp目录下创建临时文件/目录，文件/目录名类似tmp.XXXXXXXXXX，其中X是随机字符；通过-p/--tmpdir指定生成的位置，通过-t指定名字模板（包含至少3个X），例如在/tmp/test目录下生成一个*mydemo-*名称开头的临时目录，可执行命令： mktemp -d -p /tmp/test -t mydemo-XXXXXX 输出结果： /tmp/test/mydemo-sh3PlU 一条mkdir命令创建多个目录 mkdir -p dir/{subdir1,subdir2} 子目录名之间以逗号分隔，且不允许出现空格。 ","date":"2017-07-27","objectID":"/posts/2017/07/27/linux-magic-commands/:0:1","tags":["Linux"],"title":"一些不起眼却很实用的Linux命令（不定期更新）","uri":"/posts/2017/07/27/linux-magic-commands/"},{"categories":["系统工具"],"content":"文本处理 tac 反向逐行输出文件内容 最初知道这个命令时，看名字以为是tail cat的缩写，从尾部开始输出。直到一天突然发现tac不就是cat的字母反向吗？！而功能也是相反的，这应该是其真正的命名由来吧，很有意思。 paste 可以把多个文件按行进行合并，分隔符可指定 做数据统计可能会用到，比如画曲线图，作为数据源，每列数据就是一条曲线。 nl 在每行前面添加行号并输出 行号可指定格式，比如左对齐、右对齐、0填充等。 一条sed命令替换多个字符串 可以将多个表达式合并，以分号分隔 echo \"aa-bb\" | sed \"s/aa/bb/g;s/cc/dd/g\" cc-dd 也可以使用-e选项指定多个表达式 echo \"aa-bb\" | sed -e \"s/aa/cc/g\" -e \"s/bb/dd/g\" cc-dd ","date":"2017-07-27","objectID":"/posts/2017/07/27/linux-magic-commands/:0:2","tags":["Linux"],"title":"一些不起眼却很实用的Linux命令（不定期更新）","uri":"/posts/2017/07/27/linux-magic-commands/"},{"categories":["系统工具"],"content":"Vim 冒号开头的表示命令模式，非冒号开头的表示普通模式 ZZ 保存并退出 :x 保存并退出 :TOhtml 生成一个html格式，经过格式化的文本内容 这货是我一个Tab不小心敲出来的，然后把我惊到了。 ","date":"2017-07-27","objectID":"/posts/2017/07/27/linux-magic-commands/:0:3","tags":["Linux"],"title":"一些不起眼却很实用的Linux命令（不定期更新）","uri":"/posts/2017/07/27/linux-magic-commands/"},{"categories":["数据库"],"content":"有这样一个业务场景，数据量大（10T以上），数据格式是一个KV，value长度较大（100KB以上），QPS不高，读写延迟要求在5ms以内，由于内存有限，无法使用Redis/CouchBase这样的全内存KV数据库，需要寻找一个合适的数据库。 MongoDB作为一个文档数据库，可以用形如{ _id: key, field0: value }的文档存储一个KV。 当然，这里的一条数据只有两个field，对于value长度小但field数量多的大体积文档，并没有测试。 机器/系统/数据库配置 CPU Intel(R) Xeon(R) CPU E5-2650 v4 @ 2.20GHz 内存 384GB 磁盘 INTEL SSDSC2BB800G6R 操作系统 CentOS release 6.7 (Final) 内核版本 2.6.32-573.el6.x86_64 文件系统 xfs (rw,noatime,nodiratime,inode64,nobarrier) IO scheduler noop readahead 256 cacheSize 30GB 测试工具用的是YCSB，workload配置如下，数据量是100GB，文档包含两个field(_id和field0，field0的value长度是100KB)，数据随机访问（uniform），OPS 200 读写比例1:1 workload=com.yahoo.ycsb.workloads.CoreWorkload readallfields=true readproportion=0.5 updateproportion=0.5 scanproportion=0 insertproportion=0 requestdistribution=uniform # total 100GB fieldlength=102400 fieldcount=1 recordcount=1000000 operationcount=120000 threadcount=10 target=200 mongodb.url=mongodb://127.0.0.1:27017/ycsb?w=1 mongodb.batchsize=10 mongodb.writeConcern=acknowledged mongodb.readPreference=primary mongodb.maxconnections=100 mongodb.threadsAllowedToBlockForConnectionMultiplier=5 MongoDB cacheSize是30GB，80%以上触发eviction，正常有24GB数据加载到内存，缓存命中率理论值是24%，经测试，实际cache命中率是22%左右，基本吻合。 缓存命中率计算方法，观察测试前后有多少数据加载到内存，计算公式 Delta(db.serverStatus().wiredTiger.cache.“bytes read into cache”)/100KB/operationcount 另外做了热度访问测试（requestdistribution=zipfian），缓存命中率是50% 因为机器物理内存很大，Linux系统cache不可忽略，定期清除cache，测试结果更符合实际。 #! /bin/bash while true do echo \"drop caches\" free -m \u0026\u0026 sync \u0026\u0026 echo 1 \u003e /proc/sys/vm/drop_caches \u0026\u0026 free -m sleep 30 done 针对此场景的配置优化 journal日志 记录写请求，场景中写入10MB/s，将journal放到其他物理卷，减少对数据盘的IO占用。 Journal日志默认在storage.dbPath下，不支持指定其他目录，可以通过对symbol link的方式来解决，为storage.dbPath下的Journal目录创建软连接。 # mv /path/to/storage/dbpath/journal /path/another/ # ln -s /path/another/journal /path/to/storage/dbpath/ # chown -h mongod:mongod /path/to/storage/dbpath/journal Transparent Huge Pages (THP) 经测试，开启与否对性能几乎没有影响，遵从官方建议，禁用之 readahead 经测试，开启磁盘预读，读写性能有一定提升，需要注意的是，重设readahead后需要重启数据库 // 查看所有设备情况 # blockdev --report // 查看readahead # blockdev --getra /dev/sdx // 设置readahead，单位是512Bytes，256即128KB # blockdev --setra 256 /dev/sdx 关于磁盘预读，先看一下官方文档 https://docs.mongodb.com/manual/administration/production-notes/ For the WiredTiger storage engine: Set the readahead setting to 0 regardless of storage media type (spinning, SSD, etc.). Setting a higher readahead benefits sequential I/O operations. However, since MongoDB disk access patterns are generally random, setting a higher readahead provides limited benefit or performance degradation. As such, for most workloads, a readahead of 0 provides optimal MongoDB performance. In general, set the readahead setting to 0 unless testing shows a measurable, repeatable, and reliable benefit in a higher readahead value. MongoDB Professional Support can provide advice and guidance on non-zero readahead configurations. For the MMAPv1 storage engine: Ensure that readahead settings for the block devices that store the database files are appropriate. For random access use patterns, set low readahead values. A readahead of 32 (16 kB) often works well. For a standard block device, you can run sudo blockdev –report to get the readahead settings and sudo blockdev –setra to change the readahead settings. Refer to your specific operating system manual for more information. 文档表示，对于WiredTiger存储引擎，建议禁用预读（readahead设置为0）。大多数场景下MongoDB是随机访问磁盘，设置较大readahead值有利于顺序IO，提升有限，甚至影响性能。除非通过测试，确定可以带来可衡量、可重现、稳定的性能提升。同时给自家的有偿咨询服务打了一波广告，很6。 记录了readahead值分别在0/32/64/128/256下的读写延迟和负载，测试结果显示，禁用预读，读写延迟较高；启用预读，MongoDB读写性能在readahead-64时趋于稳定，在readahead-128时更佳，表现为磁盘TPS/利用率/cpu iowait相对更低。 Latency CPU iowait Disk Util Disk TPS ","date":"2017-07-24","objectID":"/posts/2017/07/24/using-mongodb-as-kv-store/:0:0","tags":["MongoDB"],"title":"一个KV存储的MongoDB使用场景","uri":"/posts/2017/07/24/using-mongodb-as-kv-store/"},{"categories":["数据库"],"content":"迁移一个分片集群，同步工具是py-mongo-sync，支持复制集同步，趁机安利一下。 跑多个sync进程，源端各shard数据同时写往目标端mongos，全量同步过程使用insert写数据。 有一个分片集合，片键是{ _id: \"hashed\" }，全量同步过程中抛出DuplicateKeyError，也就是_id重复。连接源端mongos，查询目标_id文档，读到一条数据，然后依次查询各个shard，发现在两个shard同时存在目标_id文档。 正常来讲，问题不会出在数据写入端，因为片键保证_id唯一，那么只可能是在balance过程中chunk迁移失败导致产生了孤立文档。 所以从shards-\u003emongos的同步方式，发生DuplicateKeyError时，要判断对于同一_id的多个文档，哪条是有效数据，哪条是孤立文档，如果目标端已有的是有效数据，那么忽略，否则要替换为有效数据。要做到这点，需要了解哈希索引所采用的分片规则，通过目标_id的哈希值，定位到目标shard，然后提取有效文档。当然不可忽略的一点是，数据同步前关闭源端的balancer。 还想到一个方法，从源端mongos遍历集合进行全量同步，然后再从源端各个shard去增量同步，这样可以避开孤立文档，但有一个潜在的问题，在于遍历分片集合的内部实现，如果是逐个shard遍历，那么会导致同步效率大幅下降，正所谓鱼和熊掌不可兼得。 ","date":"2017-07-07","objectID":"/posts/2017/07/07/orphaned-document-negative-effect-on-syncing-sharded-cluster/:0:0","tags":["MongoDB"],"title":"孤立文档对分片集群数据同步的影响","uri":"/posts/2017/07/07/orphaned-document-negative-effect-on-syncing-sharded-cluster/"},{"categories":["数据库"],"content":"如果没记错的话，支持WiredTiger存储引擎后，MongoDB实现了oplog并发回放。 之前凭借经验写过一个基于oplog并发回放的实时同步工具，一直想看看原生实现，以下分析参考v3.0.12源码。 核心函数是SyncTail::oplogApplication()，做了两件事： 批量读取oplog 并发回放 /* tail an oplog. ok to return, will be re-called. */ void SyncTail::oplogApplication() { ReplicationCoordinator* replCoord = getGlobalReplicationCoordinator(); while (!inShutdown()) { OpQueue ops; OperationContextImpl txn; Timer batchTimer; int lastTimeChecked = 0; do { int now = batchTimer.seconds(); // apply replication batch limits if (!ops.empty()) { if (now \u003e replBatchLimitSeconds) break; if (ops.getDeque().size() \u003e replBatchLimitOperations) break; } // occasionally check some things // (always checked in the first iteration of this do-while loop, because // ops is empty) if (ops.empty() || now \u003e lastTimeChecked) { BackgroundSync* bgsync = BackgroundSync::get(); if (bgsync-\u003egetInitialSyncRequestedFlag()) { // got a resync command return; } lastTimeChecked = now; // can we become secondary? // we have to check this before calling mgr, as we must be a secondary to // become primary tryToGoLiveAsASecondary(\u0026txn, replCoord); } const int slaveDelaySecs = replCoord-\u003egetSlaveDelaySecs().total_seconds(); if (!ops.empty() \u0026\u0026 slaveDelaySecs \u003e 0) { const BSONObj\u0026 lastOp = ops.getDeque().back(); const unsigned int opTimestampSecs = lastOp[\"ts\"]._opTime().getSecs(); // Stop the batch as the lastOp is too new to be applied. If we continue // on, we can get ops that are way ahead of the delay and this will // make this thread sleep longer when handleSlaveDelay is called // and apply ops much sooner than we like. if (opTimestampSecs \u003e static_cast\u003cunsigned int\u003e(time(0) - slaveDelaySecs)) { break; } } // keep fetching more ops as long as we haven't filled up a full batch yet } while (!tryPopAndWaitForMore(\u0026txn, \u0026ops, replCoord) \u0026\u0026 // tryPopAndWaitForMore returns // true when we need to end a // batch early (ops.getSize() \u003c replBatchLimitBytes) \u0026\u0026 !inShutdown()); // For pausing replication in tests while (MONGO_FAIL_POINT(rsSyncApplyStop)) { sleepmillis(0); } if (ops.empty()) { continue; } const BSONObj\u0026 lastOp = ops.getDeque().back(); handleSlaveDelay(lastOp); // Set minValid to the last op to be applied in this next batch. // This will cause this node to go into RECOVERING state // if we should crash and restart before updating the oplog OpTime minValid = lastOp[\"ts\"]._opTime(); setMinValid(\u0026txn, minValid); multiApply(\u0026txn, ops.getDeque()); } } ","date":"2017-04-21","objectID":"/posts/2017/04/21/mongodb-replication/:0:0","tags":["MongoDB"],"title":"MongoDB并发复制","uri":"/posts/2017/04/21/mongodb-replication/"},{"categories":["数据库"],"content":"批量读取oplog 这里用了一个do…while循环，do附加一些硬性限制以终止读取，比如读取时间超过1s、oplog数量超过5000条；while是核心逻辑，除了oplog在体积上的硬性限制（32位系统是50MB，64位系统是100MB），还有对文档操作和命令/索引操作的区分。 核心函数是SyncTail::tryPopAndWaitForMore()，从oplog缓冲队列预读一条oplog，判断其类型，如果是文档操作，返回false，继续取下一条；如果是command或索引操作，返回true，终止do…while循环，就是说文档操作可以批量回放，而命令及索引操作必须单条阻塞回放，因为此类操作的锁粒度较大（数据库锁，非文档锁）。 // check for commands if ((op[\"op\"].valuestrsafe()[0] == 'c') || // Index builds are acheived through the use of an insert op, not a command op. // The following line is the same as what the insert code uses to detect an index build. (*ns != '\\0' \u0026\u0026 nsToCollectionSubstring(ns) == \"system.indexes\")) { if (ops-\u003eempty()) { // apply commands one-at-a-time ops-\u003epush_back(op); _networkQueue-\u003econsume(); } // otherwise, apply what we have so far and come back for the command return true; } ","date":"2017-04-21","objectID":"/posts/2017/04/21/mongodb-replication/:1:0","tags":["MongoDB"],"title":"MongoDB并发复制","uri":"/posts/2017/04/21/mongodb-replication/"},{"categories":["数据库"],"content":"并发回放 SyncTail::multiApply()实现对文档操作的并发回放。 核心函数是SyncTail::fillWriterVectors()，对oplog分组。首先对oplog的namespace散列，如果存储引擎支持文档锁并且oplog类型是i/u/d，则进一步对ObjectID散列，这样保证对同一文档的操作在一个工作线程内有序回放。 SyncTail::applyOps()将各个oplog分组投递到工作线程，主进程等待回放完成，再继续取下一批oplog。 ps：所采用的散列算法是murmurhash3，特点是对于规律性较强的key，随机分布特性表现良好。 void SyncTail::fillWriterVectors(OperationContext* txn, const std::deque\u003cBSONObj\u003e\u0026 ops, std::vector\u003cstd::vector\u003cBSONObj\u003e\u003e* writerVectors) { const bool supportsDocLocking = getGlobalEnvironment()-\u003egetGlobalStorageEngine()-\u003esupportsDocLocking(); Lock::GlobalRead globalReadLock(txn-\u003elockState()); CachingCappedChecker isCapped; for (std::deque\u003cBSONObj\u003e::const_iterator it = ops.begin(); it != ops.end(); ++it) { const BSONElement e = it-\u003egetField(\"ns\"); verify(e.type() == String); const char* ns = e.valuestr(); int len = e.valuestrsize(); uint32_t hash = 0; MurmurHash3_x86_32(ns, len, 0, \u0026hash); const char* opType = it-\u003egetField(\"op\").valuestrsafe(); // For doc locking engines, include the _id of the document in the hash so we get // parallelism even if all writes are to a single collection. We can't do this for capped // collections because the order of inserts is a guaranteed property, unlike for normal // collections. if (supportsDocLocking \u0026\u0026 isCrudOpType(opType) \u0026\u0026 !isCapped(txn, ns)) { BSONElement id; switch (opType[0]) { case 'u': id = it-\u003egetField(\"o2\").Obj()[\"_id\"]; break; case 'd': case 'i': id = it-\u003egetField(\"o\").Obj()[\"_id\"]; break; } const size_t idHash = BSONElement::Hasher()(id); MurmurHash3_x86_32(\u0026idHash, sizeof(idHash), hash, \u0026hash); } (*writerVectors)[hash % writerVectors-\u003esize()].push_back(*it); } } ","date":"2017-04-21","objectID":"/posts/2017/04/21/mongodb-replication/:2:0","tags":["MongoDB"],"title":"MongoDB并发复制","uri":"/posts/2017/04/21/mongodb-replication/"},{"categories":["系统工具"],"content":"使用SecureCRT远程登录主机，空闲一段时间后，连接自动断开，提示 timed out waiting for input: auto-logout 。 查看目标主机环境变量TMOUT，如果值大于0，那么在指定时间（单位是秒）内未接收到来自终端的输入，将自动关闭终端。 禁用此功能，有两个方法： 将TMOUT设置为0 # export TMOUT=0 删除TMOUT变量 # unset TMOUT ","date":"2017-03-23","objectID":"/posts/2017/03/23/tmout/:0:0","tags":["Linux","TMOUT"],"title":"timed out waiting for input: auto-logout","uri":"/posts/2017/03/23/tmout/"},{"categories":["系统工具"],"content":"Linux下指定动态链接库查找路径有以下方法，设置环境变量或修改配置文件。 ","date":"2017-03-21","objectID":"/posts/2017/03/21/ldconfig/:0:0","tags":["Linux","ldconfig"],"title":"Linux动态链接库查找路径","uri":"/posts/2017/03/21/ldconfig/"},{"categories":["系统工具"],"content":"环境变量 设置环境变量LD_LIBRARY_PATH，当前session下立即生效，logout后失效 # export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/path/to/your/lib 可以将此命令添加到~/.bash_profile，后续login依然有效 # echo \"export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/path/to/your/lib\" \u003e\u003e ~/.bash_profile # source ~/.bash_profile ","date":"2017-03-21","objectID":"/posts/2017/03/21/ldconfig/:0:1","tags":["Linux","ldconfig"],"title":"Linux动态链接库查找路径","uri":"/posts/2017/03/21/ldconfig/"},{"categories":["系统工具"],"content":"配置文件 /etc下以ld.so开头的文件和目录，指定了动态链接库查找路径 /etc/ld.so.conf 支持include子配置文件，扩展之后，即包含一系列以逗号/冒号/TAB/换行符分隔的动态链接库目录 # cat /etc/ld.so.conf include ld.so.conf.d/*.conf /etc/ld.so.conf.d/ 动态链接库路径配置文件，结构清晰，适合为某个特定项目指定动态链接库路径 /etc/ld.so.cache 内容是动态链接库的具体关联，本身是二进制文件，可使用ldconfig读取内容 修改ld.so.conf或ld.so.conf.d下添加配置文件后，运行ldconfig使之立即生效。 ldconfig命令在以下目录内查找动态链接库，建立关联，生成cache, 即 /etc/ld.so.cache 命令行指定目录 /etc/ld.so.conf 指定目录 /lib /lib64 /usr/lib /usr/lib64 查看动态链接库关联 # ldconfig -p | less ","date":"2017-03-21","objectID":"/posts/2017/03/21/ldconfig/:0:2","tags":["Linux","ldconfig"],"title":"Linux动态链接库查找路径","uri":"/posts/2017/03/21/ldconfig/"},{"categories":["系统工具"],"content":"Zabbix配置了端口自动发现，由一个自定义脚本解析配置文件并获取端口，管理页面显示Value should be a JSON object，数据格式不正确。 在目标机运行发现脚本，输出格式正常： # mongo_proxy_discovery.sh { \"data\": [ {\"{#PORT}\": 27001} ] } 了解到使用zabbix_get可获取key的值，条件是在zabbix server端执行此命令。 尝试在开发机执行zabbix_get命令，未获取结果。 # zabbix_get -s xxx.xxx.xxx.xxx -p 10050 -k \"mongo_proxy_discovery\" 同时观察到下面zabbix_agentd日志，意思说，仅接受zabbix server端发起的连接。 29760:20170302:172754.346 failed to accept an incoming connection: connection from \"my.dev.host\" rejected, allowed hosts: \"actual.zabbix.server\" 然而我没有zabbix server的登录权限，于是尝试让开发机充当zabbix server。 yum install zabbix-server 安装完成，修改目标机zabbix_agentd配置，将Server和ServerActive设置为开发机IP，重启zabbix_agentd。 Server=my.dev.host ServerActive=my.dev.host 在开发机执行zabbix_get命令，发现问题，没有配置文件的读权限，无法获取端口。 # zabbix_get -s xxx.xxx.xxx.xxx -p 10050 -k \"mongo_proxy_discovery\" cat: /etc/mongo-proxy/test_27001.conf: Permission denied { \"data\": [ ] } 题外话，对zabbix只停留在基本使用阶段，比如模板、自动发现、自动注册等，知道zabbix是中心化服务，并不了解C/S两端如何交互，看来是S端主动poll数据。 后来了解到，调高zabbix_agentd日志级别也可以快速定位错误。 配置文件 # Specifies debug level: # 0 - no debug # 1 - critical information # 2 - error information # 3 - warnings (default) # 4 - for debugging (produces lots of information) DebugLevel=4 错误日志 15476:20170302:175926.726 for key [mongo_proxy_discovery] received value [cat: /etc/mongo-proxy/show_test_27001.conf: Permission denied { \"data\": [ ] }] 继续排查问题，检查配置文件权限，可读，没问题。 # ll /etc/mongo-proxy/test_27001.conf -rw-r--r-- 1 root root 615 Mar 1 10:43 /etc/mongo-proxy/test_27001.conf 检查所在目录权限，可读，看起来也没问题。 drw-r--r-- 2 root root 4096 Mar 2 16:43 mongo-proxy 然后与之前部署的配置目录进行对比，发现之前目录权限是755，当前是644，于是将当前目录也设置为755，竟神奇般恢复正常。 chmod 755 /etc/mongo-proxy su - zabbix -c \"cat /etc/mongo-proxy/test_27001.conf\" 这里说明一下，这钱使用纯shell脚本部署，使用mkdir创建目录，采用默认目录权限，本次使用ansible脚本部署： - name: create directory /var/log/mongo-proxy file: path: /var/log/mongo-proxy state: directory mode: 0644 那么问题来了，目录没有x权限，为什么不能读取目录下文件的内容？ 从《鸟哥的Linux私房菜》找到了答案： x (access directory)： 咦！目录的执行权限有啥用途啊？目录只是记录文件名而已，总不能拿来执行吧？没错！目录不可以被执行，目录的x代表的是用户能否进入该目录成为工作目录的用途！ 所谓的工作目录(work directory)就是你目前所在的目录啦！举例来说，当你登入Linux时， 你所在的家目录就是你当下的工作目录。而变换目录的指令是『cd』(change directory)啰！ 大致的目录权限概念是这样，底下我们来看几个范例，让你了解一下啥是目录的权限啰！ 例题： 有个目录的权限如下所示： drwxr–r– 3 root root 4096 Jun 25 08:35 .ssh 系统有个账号名称为vbird，这个账号并没有支持root群组，请问vbird对这个目录有何权限？是否可切换到此目录中？ 答： vbird对此目录仅具有r的权限，因此vbird可以查询此目录下的文件名列表。因为vbird不具有x的权限， 因此vbird并不能切换到此目录内！(相当重要的概念！) 上面这个例题中因为vbird具有r的权限，因为是r乍看之下好像就具有可以进入此目录的权限，其实那是错的。 能不能进入某一个目录，只与该目录的x权限有关啦！此外， 工作目录对于指令的执行是非常重要的，如果你在某目录下不具有x的权限， 那么你就无法切换到该目录下，也就无法执行该目录下的任何指令，即使你具有该目录的r的权限。 参考 http://cn.linux.vbird.org/linux_basic/0210filepermission.php#filepermission_dir ","date":"2017-03-06","objectID":"/posts/2017/03/06/zabbix-get-value-error/:0:0","tags":["Linux","Zabbix"],"title":"Zabbix问题排查 - Value should be a JSON object","uri":"/posts/2017/03/06/zabbix-get-value-error/"},{"categories":["数据库"],"content":"MongoDB数据复制的基础是oplog，复制集的任何数据变更都会在primary节点local.oplog.rs集合下记录oplog，secondary节点从primary持续拉取oplog并在本地回放，实现主从节点的数据实时同步。 oplog包含下列键： ts 时间戳，由unix时间戳和自增计数构成，自增计数表示该操作为当前秒内的操作顺序 h 唯一标识 v op 操作类型，其值是下列之一 i - insert u - update d - remove c - command n - no-op ns namespace，格式是database.collection o 具体操作内容，对于i，表示要写入的文档；对于u，表示要执行的变更；对于d，表示要删除的文档，即_id；对于c，表示要执行的命令 udpate操作如果只更新部分field，o键的内容是*{ $set: { … } }*；如果是replace整个文档，o键的内容是*{ _id: …, field0: … filed1: … }* o2 仅出现于update操作，用于指定要操作的目标文档，即_id 举个栗子，依次执行下列操作： demo-rs:PRIMARY\u003e use test demo-rs:PRIMARY\u003e test.coll.insert({a: 1}) demo-rs:PRIMARY\u003e test.coll.createIndex({b: 1}) demo-rs:PRIMARY\u003e test.coll.createIndex({c: 1}, {background: true}) demo-rs:PRIMARY\u003e test.coll.dropIndex('b_1') demo-rs:PRIMARY\u003e test.coll.dropIndex('b_1') demo-rs:PRIMARY\u003e test.coll.drop() 然后观察对应的oplog， demo-rs:PRIMARY\u003e use local switched to db local demo-rs:PRIMARY\u003e db.oplog.rs.find().sort({$natural: -1}) { \"ts\" : Timestamp(1482318735, 1), \"h\" : NumberLong(\"6964527823728342008\"), \"v\" : 2, \"op\" : \"c\", \"ns\" : \"test.$cmd\", \"o\" : { \"dropDatabase\" : 1 } } { \"ts\" : Timestamp(1482318723, 1), \"h\" : NumberLong(\"-6125359811671901699\"), \"v\" : 2, \"op\" : \"c\", \"ns\" : \"test.$cmd\", \"o\" : { \"deleteIndexes\" : \"coll\", \"index\" : \"c_1\" } } { \"ts\" : Timestamp(1482318662, 1), \"h\" : NumberLong(\"7320721234104966244\"), \"v\" : 2, \"op\" : \"c\", \"ns\" : \"test.$cmd\", \"o\" : { \"deleteIndexes\" : \"coll\", \"index\" : \"b_1\" } } { \"ts\" : Timestamp(1482318543, 1), \"h\" : NumberLong(\"-7326267621257646623\"), \"v\" : 2, \"op\" : \"i\", \"ns\" : \"test.system.indexes\", \"o\" : { \"ns\" : \"test.coll\", \"key\" : { \"c\" : 1 }, \"name\" : \"c_1\", \"background\" : true } } { \"ts\" : Timestamp(1482318520, 1), \"h\" : NumberLong(\"1480507733147525872\"), \"v\" : 2, \"op\" : \"i\", \"ns\" : \"test.system.indexes\", \"o\" : { \"ns\" : \"test.coll\", \"key\" : { \"b\" : 1 }, \"name\" : \"b_1\" } } { \"ts\" : Timestamp(1482318510, 2), \"h\" : NumberLong(\"1964889382590225189\"), \"v\" : 2, \"op\" : \"i\", \"ns\" : \"test.coll\", \"o\" : { \"_id\" : ObjectId(\"585a62aef6fe6ed88c29a233\"), \"a\" : 1 } } { \"ts\" : Timestamp(1482318510, 1), \"h\" : NumberLong(\"2594632055235718257\"), \"v\" : 2, \"op\" : \"c\", \"ns\" : \"test.$cmd\", \"o\" : { \"create\" : \"coll\" } } 可以看到，command的oplog类型不一定是c，比如建立索引，实际上是在目标数据库的system.indexes集合写入一条文档，类型是i。 有一点需要注意，对于类型是c的oplog，o键的值是有序的，比如 { \"ts\" : Timestamp(1482318662, 1), \"h\" : NumberLong(\"7320721234104966244\"), \"v\" : 2, \"op\" : \"c\", \"ns\" : \"test.$cmd\", \"o\" : { \"deleteIndexes\" : \"coll\", \"index\" : \"b_1\" } } 回放过程相当于执行db.runCommand({ \"deleteIndexes\" : \"coll\", \"index\" : \"b_1\" })，第一个键deleteIndexes表示操作类型是删除索引，第二个键index指定目标索引，二者要保证顺序。 在写同步工具时碰到过，pymongo将读取的文档存到一个dict，对于Python来说，dict是键序无关的，但对于MongoDB，键序不对将导致command执行失败，解决方法可参考 https://dzone.com/articles/pymongo-and-key-order 后记@20171229 如果oplog用于MongoDB间的数据同步，直接回放就行；如果MongoDB数据同步到其他数据库，update需要关注细节，特别是内嵌文档操作，字段名采用”.“点分隔的扁平化表示。 以文档{ info: { a: 1, b: 2 } }为例，insert操作的olog的o字段即原始文档；update操作的oplog相对复杂，看下面几个情景： 修改a字段 =\u003e { info: { a: 100, b: 2 } } oplog为{ op: 'u', o2: {_id: 'xxxx'}, o: { $set: { 'info.a': 100 } } } 注意 { op: 'u', o2: {_id: 'xxxx'}, o: { $set: { 'info': { 'a': 100 } } } }执行结果是{ info: { a: 100 } } 修改a\u0026b字段 =\u003e { info: { a: 100, b: 200 } } oplog为{ op: 'u', o2: {_id: 'xxxx'}, o: { $set: { 'info.a': 100, 'info.b': 200 } } } 增加c字段 =\u003e { info: { a: 1, b: 2, c:3 } } oplog为{ op: 'u', o2: {_id: 'xxxx'}, o: { $set: { 'info.c': 3 } } } 注意 { op: 'u', o2: {_id: 'xxxx'}, o: { $set: { 'info': { 'c': 3 } } } }执行结果是{ info: { c: 3 } } 删除a字段 =\u003e { info: { b: 2 } } oplog为{ op: 'u', o2: {_id: 'xxxx'}, o: { $unset: { 'info.a': true } } } info.a后面可以是任意值，不影响字段删除 注意字段名扁平化，不要使用嵌套，下面的语义是删除info字段，*{ ‘a’: true }*相当于info的值 { op: 'u', o2: {_id: 'xxxx'}, o: { $unset: { 'info': { 'a': true } } } } 等同于 { op: 'u', o2: {_id: 'xxxx'}, o: { $unset: { 'info': true } } 删除a\u0026b字段 =\u003e { info: { } } oplog为{ op: 'u', o2: {_id: 'xxxx'}, o: { $unset: { 'info.a': true, 'info.b': true } } } 删除info字段 =\u003e { } oplog为{ op: 'u', o2: {_id: 'xxxx'}, o: { $unset: { 'info': true } } 简单总结，对于update来说，$set和$unset对象的key(s)是目标操作字段。 ","date":"2016-12-24","objectID":"/posts/2016/12/24/mongodb-oplog/:0:0","tags":["MongoDB","oplog"],"title":"MongoDB oplog 分析","uri":"/posts/2016/12/24/mongodb-oplog/"},{"categories":["编程框架"],"content":"对象的序列化与序列化，可能大家更多接触的是谷歌的protobuf。 Thrift作为一个跨语言的RPC代码生成引擎，也具备此功能。 本文要说的是如何使用Thrift实现对象的序列化与反序列化，其实就是，如何以protobuf的方式使用Thrift。 Thrift描述文件： # filename: demo.thriftstructNode{1:stringhost2:i32port} 以生成的Python代码为例，Thrift生成的类型提供了两个关键方法： class Node: \"\"\" Attributes: - host - port \"\"\" thrift_spec = ( None, # 0 (1, TType.STRING, 'host', None, None, ), # 1 (2, TType.I32, 'port', None, None, ), # 2 ) def __init__(self, host=None, port=None,): self.host = host self.port = port def read(self, iprot): ... def write(self, oprot): ... read/write方法按照指定协议传输对象，所以需要一个TProtocol对象。 TProtocol对象构造时需要传入一个TTransport对象，即传输层，所以还需要一个TTransport对象。 由于数据已经准备完毕，要做的只是反序列化。 好，TMemoryBuffer满足需求。 class TMemoryBuffer(TTransportBase, CReadableTransport): \"\"\"Wraps a cStringIO object as a TTransport. NOTE: Unlike the C++ version of this class, you cannot write to it then immediately read from it. If you want to read from a TMemoryBuffer, you must either pass a string to the constructor. TODO(dreiss): Make this work like the C++ version. \"\"\" def __init__(self, value=None): \"\"\"value -- a value to read from for stringio If value is set, this will be a transport for reading, otherwise, it is for writing\"\"\" if value is not None: self._buffer = StringIO(value) else: self._buffer = StringIO() TMemoryBuffer继承TTransportBase，也属于一种TTransport，内部封装了一个StringIO对象。 利用目标数据构造一个TMemoryBuffer对象，然后调用read/write方法实现反序列化和序列化。 需要注意的是，Python在初始化TMemoryBuffer对象时必须指定value。 序列化/反序列化的示例代码： #! /usr/bin/env python # -*- coding: utf-8 -*- import sys sys.path.append('gen-py') from thrift.transport import TTransport from thrift.protocol import TBinaryProtocol from demo.ttypes import * def serialize(th_obj): \"\"\" Serialize. \"\"\" tmembuf = TTransport.TMemoryBuffer() tbinprot = TBinaryProtocol.TBinaryProtocol(tmembuf) th_obj.write(tbinprot) return tmembuf.getvalue() def deserialize(val, th_obj_type): \"\"\" Deserialize. \"\"\" th_obj = th_obj_type() tmembuf = TTransport.TMemoryBuffer(val) tbinprot = TBinaryProtocol.TBinaryProtocol(tmembuf) th_obj.read(tbinprot) return th_obj if __name__ == '__main__': node1 = Node('localhost', 8000) print 'node1:', node1 # modified node1.host = '127.0.0.1' node1.port = 9000 val = serialize(node1) node2 = deserialize(val, Node) print 'node2:', node2 输出结果： node1: Node(host='localhost', port=8000) node2: Node(host='127.0.0.1', port=9000) ","date":"2016-11-25","objectID":"/posts/2016/11/25/thrfit-serialize-and-deserialize/:0:0","tags":["Thrfit","RPC","Python"],"title":"Thrift对象的序列化与反序列化","uri":"/posts/2016/11/25/thrfit-serialize-and-deserialize/"},{"categories":["系统工具"],"content":"在这之前对logrotate理解存在误区，以为只要对目标文件配置logrotate，就可以搞定日志轮转。 生产环境下，偶有发生mongod日志为空的问题，即日志轮转后，新的mongod日志并没有写到原始日志文件，日志文件长度始终是0，此时执行mongodb自身的logRotate命令也无济于事，除非重启mongod，如果碰巧这个时间点数据库出了故障，日志都没有，搞毛线啊~~ 所以花时间了解下logrotate，项目托管在github.com/logrotate/logrotate。 由于我的logrotate配置包含copytruncate选项，即先拷贝再清空，直接看关键代码，函数调用关系是rotateLogSet() =\u003e rotateSingLog() =\u003e copyTruncate() 。 // 拷贝日志文件副本 if (sparse_copy(fdcurr, fdsave, sb, saveLog, currLog) != 1) { close(fdcurr); close(fdsave); message(MESS_ERROR, \"error copying %s to %s: %s\\n\", currLog, saveLog, strerror(errno)); unlink(saveLog); return 1; } // 清空原始日志文件 if (flags \u0026 LOG_FLAG_COPYTRUNCATE) { message(MESS_DEBUG, \"truncating %s\\n\", currLog); if (!debug) { fsync(fdsave); if (ftruncate(fdcurr, 0)) { message(MESS_ERROR, \"error truncating %s: %s\\n\", currLog, strerror(errno)); close(fdcurr); close(fdsave); return 1; } } } else message(MESS_DEBUG, \"Not truncating %s\\n\", currLog); 这里逻辑很简单，直接调用 ftruncate() 清空文件，回到文章开头的问题，mongod进程在写日志，logrotate进程执行 ftruncate()，至少logrotate没有对文件加锁，我觉得是两个进程同时写日志导致问题发生。 所以，mongodb日志轮转正确的方法是先执行mongodb自带的logRotate命令，然后利用Linux logrotate工具处理转储日志文件。 在此过程中，学习掌握了文件截断函数 truncate() 和 ftruncate() 。 还有，truncate() 重置了文件长度，如果其他进程也正在写此文件，文件偏移是怎么处理的？答案是O_APPEND，该选项使得每一次写操作，先将定位到文件末尾，以保证追加写入。 参考资料 https://linux.die.net/man/2/open https://linux.die.net/man/2/ftruncate https://github.com/logrotate/logrotate ","date":"2016-10-24","objectID":"/posts/2016/10/24/logrotate/:0:0","tags":["logrotate","MongoDB"],"title":"logrotate问题及分析","uri":"/posts/2016/10/24/logrotate/"},{"categories":["数据库"],"content":"MongoDB的WiredTiger存储引擎，用了一段时间，遇到了一些问题，通过优化WT参数，也解决了一些问题，做个小结。 cache_size 指定WT存储引擎内部cache的内存用量上限。 需要注意的是，仅作用于WiredTiger cache，而非mongod进程的内存用量上限。MongoDB同时使用WT cache和文件系统cache，往往mongod进程的内存用量高于该值。cache_size相对于物理内存总量不要设置的太满，需要留有一定内存为操作系统所用，否则有OOM潜在风险。 默认情况下，cache_used超过80%将触发eviction，如果物理内存充足，建议设置足够大的cache_size，以加载全部数据，避免不必要的eviction。 个人经验值 cache_size = (data + index) / 0.8 cache_size支持在不停服的情况下动态调整，比如将cache_size设置为80GB，执行如下命令： db.adminCommand({setParameter: 1, wiredTigerEngineRuntimeConfig: \"cache_size=80G\"}) eviction cache_used是很关键的指标，超过80%将触发eviction，类似LRU算法，淘汰冷数据，避免cache用量持续增长。 一个健康的MongoDB服务，其cache_used应该不超过80%，即使超过，能在短时间内降到80%也是没问题的。如果长时间高于80%则需要重视起来，因为cache_used过高会导致数据库性能下降，体现在慢操作（读写请求耗时长）、qr/qw高（读写请求排队）等方面。所以我们要极力保证cache_used在一个健康的范围内。 eviction包括很多参数，比如eviction_trigger、eviction_target、eviction_dirty_target等，指定在什么条件下触发和停止cache eviction，这里，我更关心的是eviction线程数量。 对于eviction线程，MongoDB默认配置是eviction=(threads_min=1,threads_max=4)，根据cache使用情况，创建1-4个eviction线程去淘汰冷数据。 如果cache体积较大、读写频繁，那么需要更多的eviction线程。 如果调高threads_max仍然无法降低cache_used，建议设置更大的cache_size 动态调整配置： db.adminCommand({setParameter: 1, wiredTigerEngineRuntimeConfig: \"eviction=(threads_min=1,threads_max=8)\"}) wiredTigerConcurrentWriteTransactions 指定最大并发写事务数。 对于写频繁的服务，通过mongostat查看运行状态，如果qw持续较高、aw经常是128（默认值），说明写请求发生排队，同时WT无法提供更高的并发写。 此时观察CPU负载，如果负载不高（相对于核数，CPU未充分利用），尝试调高此参数，能够一定程度上缓解问题，即使出现qw高，往往也是短暂的，可能下一秒恢复正常。 调高此参数，相当于压榨CPU，CPU负载较之前会有一定增加，如果负载在合理范围内，可以接受；负载过高的话，建议扩容。 建议根据实际情况动态调整，并持续观察效果，找到一个合理的值。 查看当前配置： db.adminCommand({getParameter: 1, wiredTigerConcurrentWriteTransactions: 1}) or db.adminCommand({getParameter: '*'}).wiredTigerConcurrentWriteTransactions 动态调整配置： db.adminCommand({setParameter: 1, wiredTigerConcurrentWriteTransactions: 512}) 参考 https://docs.mongodb.com/manual/reference/configuration-options/ http://source.wiredtiger.com/2.8.0/tune_cache.html http://www.developer.com/db/tips-for-mongodb-wiredtiger-performance-tuning.html ","date":"2016-08-23","objectID":"/posts/2016/08/23/mongodb-wiredtiger-performance-tuning/:0:0","tags":["MongoDB","WiredTiger","性能调优"],"title":"WiredTiger运行时参数优化","uri":"/posts/2016/08/23/mongodb-wiredtiger-performance-tuning/"},{"categories":["数据库"],"content":"背景 某服务升级，计划将原来多个v2.4复制集升级为v3.0分片集群。 ","date":"2016-06-17","objectID":"/posts/2016/06/17/sync-repl-to-sharding/:1:0","tags":["MongoDB","数据同步","复制集","分片集群"],"title":"MongoDB多个复制集到分片集群的数据同步","uri":"/posts/2016/06/17/sync-repl-to-sharding/"},{"categories":["数据库"],"content":"问题 在多个rs的增量同步过程中，发现分片集群的机器负载很高，在服务高峰时期，oplog追的慢，同步开始滞后，延迟时间有上升趋势。 整个同步过程包含两个阶段： 初始同步 目标是全部集合 insert 10w/s，大小取决于目标分片集群的性能 增量同步 目标是oplog update 3w/s，大小取决于全部源端rs的update ","date":"2016-06-17","objectID":"/posts/2016/06/17/sync-repl-to-sharding/:2:0","tags":["MongoDB","数据同步","复制集","分片集群"],"title":"MongoDB多个复制集到分片集群的数据同步","uri":"/posts/2016/06/17/sync-repl-to-sharding/"},{"categories":["数据库"],"content":"分析 机器负载高，完全不符合预期，问题出在哪里？为什么初始同步没问题，增量同步会出现性能瓶颈？ 从两个方面考虑： 增量同步的特殊性 复制集中，_id保证文档的唯一性，所有对于该文档的update/delete操作，体现在oplog，均是以_id作为条件。 分片集群 同步目标是分片集群，集合做了分片，片键（shard key）是什么？对于update/remove操作的oplog，如果片键是_id，oplog回放可以利用片键；如果片键不是_id，那么oplog回放无法利用片键。 回到问题本身，目标集合的片键非_id，对于一个update/remove操作的oplog，其回放不能直接定位到目标shard，那么该oplog将在所有shard上都执行一次，尽管这条操作仅在目标shard（包含_id指定的文档）上执行成功，但是在其他shard上会执行一次query，这里发生了不必要的性能消耗。 至于为什么insert不会有瓶颈，因为insert操作中包含文档的所有field，自然能够利用片键：） ","date":"2016-06-17","objectID":"/posts/2016/06/17/sync-repl-to-sharding/:3:0","tags":["MongoDB","数据同步","复制集","分片集群"],"title":"MongoDB多个复制集到分片集群的数据同步","uri":"/posts/2016/06/17/sync-repl-to-sharding/"},{"categories":["数据库"],"content":"解决方法 一般情况下，片键不会是_id，如果源端rs的update/remove的OPS高，可考虑分批导入到分片集群，比如新集群计划容纳10个rs，可以先导入5个rs，等业务切换完成，再导入其余5个rs。 ","date":"2016-06-17","objectID":"/posts/2016/06/17/sync-repl-to-sharding/:4:0","tags":["MongoDB","数据同步","复制集","分片集群"],"title":"MongoDB多个复制集到分片集群的数据同步","uri":"/posts/2016/06/17/sync-repl-to-sharding/"},{"categories":["系统工具"],"content":"开发机系统是CentOS 6.6，GCC版本是4.4.7，老司机们已经开着C++11上路了，话不多说，抓紧上车。 下载GCC ftp://ftp.gnu.org/gnu/gcc/gcc-4.9.4/gcc-4.9.4.tar.gz 官方FTP http://ftpmirror.gnu.org 自动选择速度较快的镜像站 下载依赖包 GMP MPC MPFR 可通过两种方式下载： 自动下载，运行GCC源码目录内置脚本： cd gcc-4.9.4 sh ./contrib/download_prerequisites 手动下载、解压，然后执行命令： cd gcc-4.9.4 ln -sf path/to/gmp-x.x.x gmp ln -sf path/to/mpc-x.x.x mpc ln -sf path/to/mpfr-x.x.x mpfr 这样，编译GCC同时自动构建上述3个库。 注意要保证3个包的源码目录是干净的，否则可能报错，必要时可执行make distclean 版本要求参考脚本*./contrib/download_prerequisites*，以4.9.4为例 # Necessary to build GCC. MPFR=mpfr-2.4.2 GMP=gmp-4.3.2 MPC=mpc-0.8.1 编译安装 官方文档强烈建议在源码目录之外，新建一个临时目录，用于编译； 个人建议设置安装目录，便于GCC多版本管理； configure选项，可以根据需求自行配置。 mkdir build-gcc-4.9.4 cd build-gcc-4.9.4 ../gcc-4.9.4/configure --prefix=/usr/local/gcc-4.9.4/ --enable-checking=release --enable-languages=c,c++ --disable-multilib make -j4 make install 设置环境变量 优先使用指定版本的GCC，可将下行代码添加到~/.bash_profile export PATH=/usr/local/gcc-4.9.4/bin:$PATH 配置libstdc++.so.6 在gcc编译目录下找到最新的libstdc++.so，拷贝至/usr/lib64/，运行命令ldconfig $ ll /usr/lib64/libstdc++.so.6 lrwxrwxrwx. 1 root root 19 Nov 20 15:59 /usr/lib64/libstdc++.so.6 -\u003e libstdc++.so.6.0.13 $ find build-gcc-4.9.4/ -name libstdc++.so.* |xargs ls -l lrwxrwxrwx. 1 root root 19 Aug 30 2016 build-gcc-4.9.4/prev-x86_64-unknown-linux-gnu/libstdc++-v3/src/.libs/libstdc++.so.6 -\u003e libstdc++.so.6.0.20 -rwxr-xr-x. 1 root root 6781772 Aug 30 2016 build-gcc-4.9.4/prev-x86_64-unknown-linux-gnu/libstdc++-v3/src/.libs/libstdc++.so.6.0.20 lrwxrwxrwx. 1 root root 19 Aug 30 2016 build-gcc-4.9.4/stage1-x86_64-unknown-linux-gnu/libstdc++-v3/src/.libs/libstdc++.so.6 -\u003e libstdc++.so.6.0.20 -rwxr-xr-x. 1 root root 6781772 Aug 30 2016 build-gcc-4.9.4/stage1-x86_64-unknown-linux-gnu/libstdc++-v3/src/.libs/libstdc++.so.6.0.20 lrwxrwxrwx. 1 root root 19 Aug 30 2016 build-gcc-4.9.4/x86_64-unknown-linux-gnu/libstdc++-v3/src/.libs/libstdc++.so.6 -\u003e libstdc++.so.6.0.20 -rwxr-xr-x. 1 root root 6781772 Aug 30 2016 build-gcc-4.9.4/x86_64-unknown-linux-gnu/libstdc++-v3/src/.libs/libstdc++.so.6.0.20 $ cp build-gcc-4.9.4/x86_64-unknown-linux-gnu/libstdc++-v3/src/.libs/libstdc++.so.6.0.20 /usr/lib64/ $ ldconfig $ ll /usr/lib64/libstdc++.so.6 lrwxrwxrwx. 1 root root 19 Nov 20 16:12 /usr/lib64/libstdc++.so.6 -\u003e libstdc++.so.6.0.20 好了，开车上路吧! 参考资料 https://gcc.gnu.org/install/ ","date":"2016-05-04","objectID":"/posts/2016/05/04/installing-gcc/:0:0","tags":["Linux","GCC"],"title":"Linux下编译安装GCC 4.9.4","uri":"/posts/2016/05/04/installing-gcc/"},{"categories":["数据库"],"content":"最近写mongo-proxy时，注意到了ntoreturn，看完官方文档，有点迷糊，因为它和batchSize似乎干着同一件事；之后看了源码，终于弄清楚二者的关系，对MongoDB加深了理解。 首先需要了解一些基本知识： 对于数据查询，结果集可能会很大，db会把结果集划分成多个部分，分批传输至client。 对于MongoDB，ntoreturn和batchSize影响第一批结果的文档数量。 MongoDB query定义如下： struct OP_QUERY { MsgHeader header; // standard message header int32 flags; // bit vector of query options. See below for details. cstring fullCollectionName; // \"dbname.collectionname\" int32 numberToSkip; // number of documents to skip int32 numberToReturn; // number of documents to return // in the first OP_REPLY batch document query; // query object. See below for details. [ document returnFieldsSelector; ] // Optional. Selector indicating the fields // to return. See below for details. } 官方文档对于numberToReturn的说明如下： Limits the number of documents in the first OP_REPLY message to the query. However, the database will still establish a cursor and return the cursorID to the client if there are more results than numberToReturn. If the client driver offers ‘limit’ functionality (like the SQL LIMIT keyword), then it is up to the client driver to ensure that no more than the specified number of document are returned to the calling application. If numberToReturn is 0, the db will use the default return size. If the number is negative, then the database will return that number and close the cursor. No further results for that query can be fetched. If numberToReturn is 1 the server will treat it as -1 (closing the cursor automatically). numberToReturn限制第一批返回结果的文档数量，其值可以是： 正整数 返回指定数量的目标文档，如果结果集文档数量多于numberToReturn，数据库内部会建立一个cursor，并把cursorID返回至客户端，以供其读取后续的文档 1是例外，MongoDB会将1视为-1，返回文档并关闭cursor 0 返回默认数量的目标文档 负整数 返回指定数量的目标文档，然后关闭cursor，因此无法获取后续的文档 需要特别指出，command是特殊的query，command的numberToReturn只允许是1或-1。默认情况下，如果一个cursor全部读完或空闲600秒后，server会将其关闭，cursor是数据库的一类资源，应避免产生过多空闲的cursor。 读到这里，问题来了，它和batchSize的作用差不多，下面具体分析第一批结果的组装过程： src/mongo/db/query/find.cpp // Run the query. // bb is used to hold query results // this buffer should contain either requested documents per query or // explain information, but not both BufBuilder bb(FindCommon::kInitReplyBufferSize); bb.skip(sizeof(QueryResult::Value)); // How many results have we obtained from the executor? int numResults = 0; // If we're replaying the oplog, we save the last time that we read. Timestamp slaveReadTill; BSONObj obj; PlanExecutor::ExecState state; // uint64_t numMisplacedDocs = 0; // Get summary info about which plan the executor is using. { stdx::lock_guard\u003cClient\u003e lk(*txn-\u003egetClient()); curop.setPlanSummary_inlock(Explain::getPlanSummary(exec.get())); } while (PlanExecutor::ADVANCED == (state = exec-\u003egetNext(\u0026obj, NULL))) { // Add result to output buffer. bb.appendBuf((void*)obj.objdata(), obj.objsize()); // Count the result. ++numResults; // Possibly note slave's position in the oplog. if (pq.isOplogReplay()) { BSONElement e = obj[\"ts\"]; if (Date == e.type() || bsonTimestamp == e.type()) { slaveReadTill = e.timestamp(); } } if (FindCommon::enoughForFirstBatch(pq, numResults, bb.len())) { LOG(5) \u003c\u003c \"Enough for first batch, wantMore=\" \u003c\u003c pq.wantMore() \u003c\u003c \" ntoreturn=\" \u003c\u003c pq.getNToReturn().value_or(0) \u003c\u003c \" numResults=\" \u003c\u003c numResults \u003c\u003c endl; break; } } 上面这段代码的功能是组装第一批返回的结果集，每填充一个文档，检查一次是否满足返回条件（L39），如果满足，则跳出循环。 看一下FindCommon::enoughForFirstBatch() src/mongo/db/query/find_common.cpp bool FindCommon::enoughForFirstBatch(const LiteParsedQuery\u0026 pq, long long numDocs, int bytesBuffered) { if (!pq.getEffectiveBatchSize()) { // If there is no batch size, we stop generating additional results as soon as we have // either 101 documents or at least 1MB of data. return (bytesBuffered \u003e 1024 * 1024) || numDocs \u003e= LiteParsedQuery::kDefaultBatchSize; } // If there is a batch size, we add results until either satisfying this batch size or exceeding // the 4MB size threshold. return numDocs \u003e= pq.getEffectiveBatchSize().value() || bytesBuffered \u003e kMaxBytesToReturnToClientAtOnce; } src/mongo/db/query/lite_parsed_query.cpp const long long LiteParsedQuery::kDefaultBatchSize = 101; src/mongo/db","date":"2015-11-02","objectID":"/posts/2015/11/02/ntoreturn-and-batchsize/:0:0","tags":["MongoDB","ntoreturn","batchSize"],"title":"MongoDB之浅谈ntoreturn和batchSize","uri":"/posts/2015/11/02/ntoreturn-and-batchsize/"},{"categories":["系统工具"],"content":"博主作为一枚Vim用户兼粉丝，早期安装taglist，下载插件，然后手工拷贝到指定目录，还没觉得麻烦，直到发现Vundle这一神器，才认识到自己过去有多么土。 Vundle是一个Vim插件管理器，名字是Vim Bundle的缩写，谐音“豌豆”。 Vundle具备以下功能： 在.vimrc里配置待安装的插件 安装插件 更新插件 根据名称查找可用插件 清理未使用的插件 在Vim命令模式下执行上述功能 与此同时，Vundle自行管理插件目录；对安装或更新的插件，自动生成帮助文档。 下面说一下Vundle的安装及使用，首先，确保你的Linux系统安装了以下工具： Git 默认将插件git clone到*~/.vim/bundle/*目录下 Curl 用于查找插件 用法： 安装Vundle $ git clone https://github.com/VundleVim/Vundle.vim.git ~/.vim/bundle/Vundle.vim 配置插件 以下内容拷贝至.vimrc起始处，部分插件仅用于演示，不需要的可以删除 set nocompatible \" be iMproved, required filetype off \" required \" set the runtime path to include Vundle and initialize set rtp+=~/.vim/bundle/Vundle.vim call vundle#begin() \" alternatively, pass a path where Vundle should install plugins \"call vundle#begin('~/some/path/here') \" let Vundle manage Vundle, required Plugin 'VundleVim/Vundle.vim' \" The following are examples of different formats supported. \" Keep Plugin commands between vundle#begin/end. \" plugin on GitHub repo Plugin 'tpope/vim-fugitive' \" plugin from http://vim-scripts.org/vim/scripts.html Plugin 'L9' \" Git plugin not hosted on GitHub Plugin 'git://git.wincent.com/command-t.git' \" git repos on your local machine (i.e. when working on your own plugin) Plugin 'file:///home/gmarik/path/to/plugin' \" The sparkup vim script is in a subdirectory of this repo called vim. \" Pass the path to set the runtimepath properly. Plugin 'rstacruz/sparkup', {'rtp': 'vim/'} \" Avoid a name conflict with L9 Plugin 'user/L9', {'name': 'newL9'} \" All of your Plugins must be added before the following line call vundle#end() \" required filetype plugin indent on \" required \" To ignore plugin indent changes, instead use: \"filetype plugin on \" \" Brief help \" :PluginList - lists configured plugins \" :PluginInstall - installs plugins; append `!` to update or just :PluginUpdate \" :PluginSearch foo - searches for foo; append `!` to refresh local cache \" :PluginClean - confirms removal of unused plugins; append `!` to auto-approve removal \" \" see :h vundle for more details or wiki for FAQ \" Put your non-Plugin stuff after this line 安装插件 启动Vim，运行:PluginInstall 关于Vundle详细用法，可参考帮助文档，Vim命令模式下输入:help vundle。 ","date":"2015-09-24","objectID":"/posts/2015/09/24/vundle/:0:0","tags":["Vim"],"title":"Vim插件管理器 - Vundle","uri":"/posts/2015/09/24/vundle/"}]